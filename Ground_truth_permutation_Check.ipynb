{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle as p\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "import sys\n",
    "import slam.io as sio\n",
    "import networkx as nx\n",
    "import tools.graph_visu as gv\n",
    "import tools.graph_processing as gp\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import trimesh\n",
    "import slam.topology as stop\n",
    "from sphere import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sphere_random_sampling(vertex_number=100, radius=1.0):\n",
    "    \"\"\"\n",
    "    generate a sphere with random sampling\n",
    "    :param vertex_number: number of vertices in the output spherical mesh\n",
    "    :param radius: radius of the output sphere\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    coords = np.zeros((vertex_number, 3))\n",
    "    for i in range(vertex_number):\n",
    "        M = np.random.normal(size=(3, 3))\n",
    "        Q, R = np.linalg.qr(M)\n",
    "        coords[i, :] = Q[:, 0].transpose() * np.sign(R[0, 0])\n",
    "    if radius != 1:\n",
    "        coords = radius * coords\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tri_from_hull(vertices):\n",
    "    \"\"\"\n",
    "    compute faces from vertices using trimesh convex hull\n",
    "    :param vertices: (n, 3) float\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mesh = trimesh.Trimesh(vertices=vertices, process=False)\n",
    "    return mesh.convex_hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_graph(original_graph, nb_vertices, sigma_noise_nodes = 1, sigma_noise_edges = 1, radius = 100):\n",
    "    \n",
    "    # generate ground_truth permutation\n",
    "    ground_truth_permutation = np.random.permutation(nb_vertices)\n",
    "    #ground_truth_permutation = np.arange(nb_vertices)\n",
    "    \n",
    "    # create a new graph\n",
    "    noisy_graph = nx.Graph()\n",
    "\n",
    "    # add the nodes (not very clean but it works fine and run in no time)\n",
    "    for node_to_add in range(len(ground_truth_permutation)):\n",
    "        for original_node, current_node in enumerate(ground_truth_permutation):\n",
    "            if current_node == node_to_add:\n",
    "                print(original_node,node_to_add)\n",
    "                noisy_coordinate = original_graph.nodes[original_node][\"coord\"] + \\\n",
    "                    np.random.multivariate_normal(np.zeros(3), np.eye(3) * sigma_noise_nodes)\n",
    "\n",
    "                # We project on the sphere\n",
    "                noisy_coordinate = noisy_coordinate / np.linalg.norm(noisy_coordinate) * radius\n",
    "                noisy_graph.add_node(node_to_add, coord = noisy_coordinate)\n",
    "        \n",
    "    compute_noisy_edges = tri_from_hull(list(nx.get_node_attributes(noisy_graph,'coord').values())) # take all peturbated coord and comp conv hull.\n",
    "    adja = stop.edges_to_adjacency_matrix(compute_noisy_edges) # compute the new adjacency mat\n",
    "    edge_list = [tuple((u,v)) for u,v in zip(adja.nonzero()[0],adja.nonzero()[1])] # convert to edge list to add edge attributes.\n",
    "\n",
    "\n",
    "    # add the edges\n",
    "    for edge in edge_list:\n",
    "\n",
    "        # get the original and corresponding ends\n",
    "        #end_a_corresponding, end_b_corresponding = ground_truth_permutation[edge[0]], ground_truth_permutation[edge[1]]\n",
    "        coordinate_a, coordinate_b = noisy_graph.nodes[edge[0]][\"coord\"], noisy_graph.nodes[edge[1]][\"coord\"]\n",
    "\n",
    "        # calculate noisy geodesic distance\n",
    "        noisy_geodesic_dist = gp.compute_geodesic_distance_sphere(coordinate_a, coordinate_b, radius)\n",
    "\n",
    "        # Add the new edge to the graph\n",
    "        noisy_graph.add_edge(edge[0],edge[1], weight = 1.0, geodesic_distance = noisy_geodesic_dist)\n",
    "    \n",
    "    return ground_truth_permutation, noisy_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_graph_2(original_graph, nb_vertices,nb_outliers,sigma_noise_nodes=1, sigma_noise_edges=1, radius=100):\n",
    "    # Perturbate the coordinates\n",
    "    # noisy_coord = [points+np.random.multivariate_normal(np.zeros(3), np.eye(3) * sigma_noise_nodes)\n",
    "    # \tfor points in list(nx.get_node_attributes(original_graph,'coord').values())]\n",
    "\n",
    "    ground_truth_permutation = np.arange(nb_vertices)\n",
    "    # create a new graph\n",
    "    noisy_graph = nx.Graph()\n",
    "    # add the nodes (not very clean but it works fine and run in no time)\n",
    "    for node_to_add in range(len(ground_truth_permutation)):\n",
    "        for original_node, current_node in enumerate(ground_truth_permutation):\n",
    "            if current_node == node_to_add:\n",
    "                # noisy_coordinate = original_graph.nodes[original_node][\"coord\"] + \\\n",
    "                # \tnp.random.multivariate_normal(np.zeros(3), np.eye(3) * sigma_noise_nodes)\n",
    "\n",
    "                # Sampling from Von Mises - Fisher distribution\n",
    "                original_coord = original_graph.nodes[original_node][\"coord\"]\n",
    "                mean_original = original_coord / np.linalg.norm(original_coord)  # convert to mean vector\n",
    "                noisy_coordinate = Sphere().sample(1, distribution='vMF', mu=mean_original,\n",
    "                                                   kappa=sigma_noise_nodes).sample[0]\n",
    "\n",
    "                #noisy_coordinate = noisy_coordinate / np.linalg.norm(noisy_coordinate) * radius\n",
    "                noisy_coordinate  = noisy_coordinate * np.linalg.norm(original_coord)\n",
    "                noisy_graph.add_node(node_to_add, coord=noisy_coordinate)\n",
    "                \n",
    "    noisy_coord = list(nx.get_node_attributes(noisy_graph, 'coord').values())\n",
    "                \n",
    "    # Add Outliers\n",
    "    if nb_outliers > 0:\n",
    "        print(\"nb_outliers: \",nb_outliers)\n",
    "        sphere_random_sampling = generate_sphere_random_sampling(vertex_number=nb_outliers, radius=radius)\n",
    "        # merge pertubated and outlier coordinates to add edges \n",
    "        all_coord = noisy_coord + list(sphere_random_sampling)\n",
    "    else:\n",
    "        all_coord = noisy_coord\n",
    "\n",
    "    print(\"nb_outliers: \",nb_outliers)\n",
    "\n",
    "\n",
    "    compute_noisy_edges = tri_from_hull(all_coord)  # take all peturbated coord and comp conv hull.\n",
    "    adja = stop.edges_to_adjacency_matrix(compute_noisy_edges)  # compute the new adjacency mat.\n",
    "\n",
    "    # Extracting the ground-truth correspondence\n",
    "    ground_truth_permutation = []\n",
    "    for ar1 in compute_noisy_edges.vertices.view(np.ndarray):\n",
    "        index = 0\n",
    "        for ar2 in noisy_coord:\n",
    "            if np.mean(ar1) == np.mean(ar2):\n",
    "                ground_truth_permutation.append(index)\n",
    "                index += 1\n",
    "                break\n",
    "            else:\n",
    "                index += 1\n",
    "                continue\n",
    "\n",
    "    print(\"Total Ground truth nodes: \", len(ground_truth_permutation))\n",
    "    print(\"Total number of nodes : \", len(compute_noisy_edges.vertices))\n",
    "\n",
    "    noisy_graph = nx.from_numpy_matrix(adja.todense())\n",
    "\n",
    "    node_attribute_dict = {}\n",
    "    for node in noisy_graph.nodes():\n",
    "        node_attribute_dict[node] = {\"coord\": np.array(compute_noisy_edges.vertices[node])}\n",
    "\n",
    "    nx.set_node_attributes(noisy_graph, node_attribute_dict)\n",
    "\n",
    "    nx.set_edge_attributes(noisy_graph, 1.0, name=\"weight\")\n",
    "\n",
    "    edge_attribute_dict = {}\n",
    "    id_counter = 0  # useful for affinity matrix caculation\n",
    "    for edge in noisy_graph.edges:\n",
    "        # We calculate the geodesic distance\n",
    "        end_a = noisy_graph.nodes()[edge[0]][\"coord\"]\n",
    "        end_b = noisy_graph.nodes()[edge[1]][\"coord\"]\n",
    "        geodesic_dist = gp.compute_geodesic_distance_sphere(end_a, end_b, radius)\n",
    "\n",
    "        # add the information in the dictionnary\n",
    "        edge_attribute_dict[edge] = {\"geodesic_distance\": geodesic_dist, \"id\": id_counter}\n",
    "        id_counter += 1\n",
    "\n",
    "    # add the edge attributes to the graph\n",
    "    nx.set_edge_attributes(noisy_graph, edge_attribute_dict)\n",
    "\n",
    "    return np.array(ground_truth_permutation), noisy_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_graph_3(original_graph, nb_vertices,nb_outliers,sigma_noise_nodes=1, sigma_noise_edges=1, radius=100):\n",
    "    # Perturbate the coordinates\n",
    "    \n",
    "    noisy_coord = []\n",
    "    key = [] \n",
    "    value = []\n",
    "    \n",
    "    for index in range(nb_vertices):\n",
    "        \n",
    "        # Sampling from Von Mises - Fisher distribution\n",
    "        original_coord = original_graph.nodes[index][\"coord\"]\n",
    "        mean_original = original_coord / np.linalg.norm(original_coord)  # convert to mean vector\n",
    "        noisy_coordinate = Sphere().sample(1, distribution='vMF', mu=mean_original,\n",
    "                                           kappa=sigma_noise_nodes).sample[0]\n",
    "\n",
    "        noisy_coordinate  = noisy_coordinate * np.linalg.norm(original_coord)\n",
    "        #print(noisy_coordinate)\n",
    "        noisy_coord.append(noisy_coordinate)\n",
    "                   \n",
    "    # Add Outliers\n",
    "    sphere_random_sampling = []\n",
    "    if nb_outliers > 0:\n",
    "        print(\"nb_outliers: \",nb_outliers)\n",
    "        sphere_random_sampling = generate_sphere_random_sampling(vertex_number=nb_outliers, radius=radius)\n",
    "        # merge pertubated and outlier coordinates to add edges \n",
    "        all_coord = noisy_coord + list(sphere_random_sampling)\n",
    "    else:\n",
    "        all_coord = noisy_coord\n",
    "\n",
    "    print(\"nb_outliers: \",nb_outliers)\n",
    "\n",
    "    compute_noisy_edges = tri_from_hull(all_coord)  # take all peturbated coord and comp conv hull.\n",
    "    adja = stop.edges_to_adjacency_matrix(compute_noisy_edges)  # compute the new adjacency mat.\n",
    "\n",
    "    # Extracting the ground-truth correspondence\n",
    "\n",
    "    noisy_graph = nx.from_numpy_matrix(adja.todense())\n",
    "    \n",
    "    print(\"all nodes: \",len(noisy_graph.nodes))\n",
    "\n",
    "    node_attribute_dict = {}\n",
    "    for node in noisy_graph.nodes():\n",
    "        node_attribute_dict[node] = {\"coord\": np.array(compute_noisy_edges.vertices[node])}\n",
    "\n",
    "    nx.set_node_attributes(noisy_graph, node_attribute_dict)\n",
    "\n",
    "    nx.set_edge_attributes(noisy_graph, 1.0, name=\"weight\")\n",
    "\n",
    "    edge_attribute_dict = {}\n",
    "    id_counter = 0  # useful for affinity matrix caculation\n",
    "    for edge in noisy_graph.edges:\n",
    "        # We calculate the geodesic distance\n",
    "        end_a = noisy_graph.nodes()[edge[0]][\"coord\"]\n",
    "        end_b = noisy_graph.nodes()[edge[1]][\"coord\"]\n",
    "        geodesic_dist = gp.compute_geodesic_distance_sphere(end_a, end_b, radius)\n",
    "\n",
    "        # add the information in the dictionnary\n",
    "        edge_attribute_dict[edge] = {\"geodesic_distance\": geodesic_dist, \"id\": id_counter}\n",
    "        id_counter += 1\n",
    "\n",
    "    # add the edge attributes to the graph\n",
    "    nx.set_edge_attributes(noisy_graph, edge_attribute_dict)\n",
    "    \n",
    "    ground_truth_permutation = []\n",
    "\n",
    "    for ar1 in noisy_coord:\n",
    "        for i in range(len(noisy_graph.nodes)):\n",
    "            if np.mean(ar1) == np.mean(noisy_graph.nodes[i]['coord']):    \n",
    "                if i >=20:\n",
    "#                     print(\"Key\",i)\n",
    "                    key.append(i)\n",
    "                ground_truth_permutation.append(i)\n",
    "                break\n",
    "                \n",
    "    for outlier in sphere_random_sampling:\n",
    "        for i in range(len(noisy_graph.nodes)):\n",
    "            if np.mean(noisy_graph.nodes[i]['coord']) == np.mean(outlier):\n",
    "                if i<20:\n",
    "                    value.append(i)\n",
    "#                     print(\"value\",i)\n",
    "                \n",
    "    if nb_outliers > 0:\n",
    "        index = 0\n",
    "        for j in range(len(ground_truth_permutation)):\n",
    "            if ground_truth_permutation[j] == key[index]:\n",
    "                ground_truth_permutation[j] = value[index]\n",
    "                index+=1\n",
    "                if index == len(key):\n",
    "                    break\n",
    "                    \n",
    "    key = key + value\n",
    "    value = value + key\n",
    "    \n",
    "    mapping = dict(zip(key,value))\n",
    "    print(mapping)\n",
    "\n",
    "    noisy_graph = nx.relabel_nodes(noisy_graph, mapping)\n",
    "\n",
    "    return ground_truth_permutation,noisy_graph,noisy_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_gpickle(\"reference_0.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_coord =  list(nx.get_node_attributes(G,'coord').values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ground_truth_permutation_old, noisy_graph_old = generate_noisy_graph(G,len(original_coord),10,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy_graph_old_coord = list(nx.get_node_attributes(noisy_graph_old,'coord').values())\n",
    "# noisy_graph_old_coord[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ground_truth_permutation_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground_truth_permutation, noisy_graph = generate_noisy_graph_2(G,len(original_coord),0,10000,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy_graph_coord = list(nx.get_node_attributes(noisy_graph,'coord').values())\n",
    "# noisy_graph_coord[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_outliers:  4\n",
      "nb_outliers:  4\n",
      "all nodes:  24\n",
      "{20: 9, 23: 13, 21: 10, 22: 16, 9: 20, 13: 23, 10: 21, 16: 22}\n"
     ]
    }
   ],
   "source": [
    "ground_truth, noisy_graph,noisy_coord = generate_noisy_graph_3(G,len(original_coord),4,10,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(noisy_graph.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeView((0, 1, 2, 20, 4, 5, 22, 7, 8, 9, 24, 11, 12, 13, 14, 15, 16, 17, 18, 19, 3, 21, 6, 23, 10))"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 89.37296722 -43.42786467 -11.24692408] [ 89.37296722 -43.42786467 -11.24692408]\n",
      "[-70.68124033 -61.88920231  34.26206215] [-70.68124033 -61.88920231  34.26206215]\n",
      "[53.24682498 29.71840425 79.2564955 ] [53.24682498 29.71840425 79.2564955 ]\n",
      "[ 72.84140935 -64.61771082  22.7745589 ] [ 72.84140935 -64.61771082  22.7745589 ]\n",
      "[-59.42440855  -2.15333201 -80.39964446] [-59.42440855  -2.15333201 -80.39964446]\n",
      "[ -4.03898414 -96.10700753 -27.33367358] [ -4.03898414 -96.10700753 -27.33367358]\n",
      "[-98.70326713 -15.96603748  -1.65852511] [-98.70326713 -15.96603748  -1.65852511]\n",
      "[ 39.11551637  80.70289667 -44.23820575] [ 39.11551637  80.70289667 -44.23820575]\n",
      "[-68.16044975 -71.4933573  -15.58373994] [-68.16044975 -71.4933573  -15.58373994]\n",
      "[21.49964697 94.08992727 26.16965353] [21.49964697 94.08992727 26.16965353]\n",
      "[-18.26719043 -94.98477029  25.38115771] [-18.26719043 -94.98477029  25.38115771]\n",
      "[-17.9530495   96.09616462 -21.0526758 ] [-17.9530495   96.09616462 -21.0526758 ]\n",
      "[-20.62593061  90.47712897 -37.26204665] [-20.62593061  90.47712897 -37.26204665]\n",
      "[-30.27504253 -65.50641324  69.22666845] [-30.27504253 -65.50641324  69.22666845]\n",
      "[-19.37846857 -30.34833188  93.29230251] [-19.37846857 -30.34833188  93.29230251]\n",
      "[-17.55643254   2.88002083 -98.40466024] [-17.55643254   2.88002083 -98.40466024]\n",
      "[-71.04734813  11.86954884  69.36417039] [-71.04734813  11.86954884  69.36417039]\n",
      "[-84.89312188 -19.26122148  49.21547728] [-84.89312188 -19.26122148  49.21547728]\n",
      "[-65.50351097 -75.55629518  -0.7323317 ] [-65.50351097 -75.55629518  -0.7323317 ]\n",
      "[-41.44131246  63.58995313  65.10710777] [-41.44131246  63.58995313  65.10710777]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "for i in ground_truth:\n",
    "    print(noisy_graph.nodes[i]['coord'],noisy_coord[index])\n",
    "    index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_noisy_edges = tri_from_hull(noisy_coord_3+list(outlier_coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: 21\n",
      "key: 23\n",
      "key: 20\n",
      "key: 22\n",
      "value: 6\n",
      "value: 10\n",
      "value: 2\n",
      "value: 0\n"
     ]
    }
   ],
   "source": [
    "ground_truth_sequence = []\n",
    "\n",
    "coords_new = compute_noisy_edges.vertices.view(np.ndarray)\n",
    "\n",
    "key = []\n",
    "value = []\n",
    "\n",
    "for ar1 in noisy_coord_3:\n",
    "    for i in range(len(coords_new)):\n",
    "        if np.mean(ar1) == np.mean(coords_new[i]):\n",
    "            if i >=20:\n",
    "                key.append(i)\n",
    "                print(\"key:\",i)\n",
    "            ground_truth_sequence.append(i)\n",
    "            \n",
    "            \n",
    "for outlier in outlier_coords:\n",
    "    for i in range(len(coords_new)):\n",
    "        if np.mean(coords_new[i]) == np.mean(outlier):\n",
    "            if i<20:\n",
    "                value.append(i)\n",
    "                print(\"value:\",i)\n",
    "                \n",
    "index = 0\n",
    "for j in range(len(ground_truth_sequence)):\n",
    "    if ground_truth_sequence[j] == key[index]:\n",
    "        ground_truth_sequence[j] = value[index]\n",
    "        index+=1\n",
    "                \n",
    "mapping = dict(zip(key,value))\n",
    "\n",
    "noisy_graph_3 = nx.relabel_nodes(noisy_graph_3, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 11, 7, 9, 12, 1, 4, 6, 10, 15, 8, 17, 19, 13, 18, 2, 16, 3, 14, 0]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-35.58280458,  86.04214399,  36.47757498])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_coord_3[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coord': array([49.16931926, 81.21885571, 31.39865475])}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_graph_3.nodes[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'coord': array([-35.58280458,  86.04214399,  36.47757498])}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_G.nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 11, 7, 9, 12, 1, 4, 6, 23, 15, 8, 17, 19, 13, 18, 20, 16, 3, 14, 22]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_G = noisy_graph_3.copy()\n",
    "\n",
    "test_G = nx.relabel_nodes(test_G, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

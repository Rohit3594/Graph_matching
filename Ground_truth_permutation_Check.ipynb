{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle as p\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "import sys\n",
    "import slam.io as sio\n",
    "import networkx as nx\n",
    "import tools.graph_visu as gv\n",
    "import tools.graph_processing as gp\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import trimesh\n",
    "import slam.topology as stop\n",
    "from sphere import *\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sphere_random_sampling(vertex_number=100, radius=1.0):\n",
    "    \"\"\"\n",
    "    generate a sphere with random sampling\n",
    "    :param vertex_number: number of vertices in the output spherical mesh\n",
    "    :param radius: radius of the output sphere\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    coords = np.zeros((vertex_number, 3))\n",
    "    for i in range(vertex_number):\n",
    "        M = np.random.normal(size=(3, 3))\n",
    "        Q, R = np.linalg.qr(M)\n",
    "        coords[i, :] = Q[:, 0].transpose() * np.sign(R[0, 0])\n",
    "    if radius != 1:\n",
    "        coords = radius * coords\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tri_from_hull(vertices):\n",
    "    \"\"\"\n",
    "    compute faces from vertices using trimesh convex hull\n",
    "    :param vertices: (n, 3) float\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    mesh = trimesh.Trimesh(vertices=vertices, process=False)\n",
    "    return mesh.convex_hull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_noisy_graph(original_graph, nb_vertices, sigma_noise_nodes = 1, sigma_noise_edges = 1, radius = 100):\n",
    "    \n",
    "#     # generate ground_truth permutation\n",
    "#     ground_truth_permutation = np.random.permutation(nb_vertices)\n",
    "#     #ground_truth_permutation = np.arange(nb_vertices)\n",
    "    \n",
    "#     # create a new graph\n",
    "#     noisy_graph = nx.Graph()\n",
    "\n",
    "#     # add the nodes (not very clean but it works fine and run in no time)\n",
    "#     for node_to_add in range(len(ground_truth_permutation)):\n",
    "#         for original_node, current_node in enumerate(ground_truth_permutation):\n",
    "#             if current_node == node_to_add:\n",
    "#                 print(original_node,node_to_add)\n",
    "#                 noisy_coordinate = original_graph.nodes[original_node][\"coord\"] + \\\n",
    "#                     np.random.multivariate_normal(np.zeros(3), np.eye(3) * sigma_noise_nodes)\n",
    "\n",
    "#                 # We project on the sphere\n",
    "#                 noisy_coordinate = noisy_coordinate / np.linalg.norm(noisy_coordinate) * radius\n",
    "#                 noisy_graph.add_node(node_to_add, coord = noisy_coordinate)\n",
    "        \n",
    "#     compute_noisy_edges = tri_from_hull(list(nx.get_node_attributes(noisy_graph,'coord').values())) # take all peturbated coord and comp conv hull.\n",
    "#     adja = stop.edges_to_adjacency_matrix(compute_noisy_edges) # compute the new adjacency mat\n",
    "#     edge_list = [tuple((u,v)) for u,v in zip(adja.nonzero()[0],adja.nonzero()[1])] # convert to edge list to add edge attributes.\n",
    "\n",
    "\n",
    "#     # add the edges\n",
    "#     for edge in edge_list:\n",
    "\n",
    "#         # get the original and corresponding ends\n",
    "#         #end_a_corresponding, end_b_corresponding = ground_truth_permutation[edge[0]], ground_truth_permutation[edge[1]]\n",
    "#         coordinate_a, coordinate_b = noisy_graph.nodes[edge[0]][\"coord\"], noisy_graph.nodes[edge[1]][\"coord\"]\n",
    "\n",
    "#         # calculate noisy geodesic distance\n",
    "#         noisy_geodesic_dist = gp.compute_geodesic_distance_sphere(coordinate_a, coordinate_b, radius)\n",
    "\n",
    "#         # Add the new edge to the graph\n",
    "#         noisy_graph.add_edge(edge[0],edge[1], weight = 1.0, geodesic_distance = noisy_geodesic_dist)\n",
    "    \n",
    "#     return ground_truth_permutation, noisy_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_noisy_graph_2(original_graph, nb_vertices,nb_outliers,sigma_noise_nodes=1, sigma_noise_edges=1, radius=100):\n",
    "#     # Perturbate the coordinates\n",
    "#     # noisy_coord = [points+np.random.multivariate_normal(np.zeros(3), np.eye(3) * sigma_noise_nodes)\n",
    "#     # \tfor points in list(nx.get_node_attributes(original_graph,'coord').values())]\n",
    "\n",
    "#     ground_truth_permutation = np.arange(nb_vertices)\n",
    "#     # create a new graph\n",
    "#     noisy_graph = nx.Graph()\n",
    "#     # add the nodes (not very clean but it works fine and run in no time)\n",
    "#     for node_to_add in range(len(ground_truth_permutation)):\n",
    "#         for original_node, current_node in enumerate(ground_truth_permutation):\n",
    "#             if current_node == node_to_add:\n",
    "#                 # noisy_coordinate = original_graph.nodes[original_node][\"coord\"] + \\\n",
    "#                 # \tnp.random.multivariate_normal(np.zeros(3), np.eye(3) * sigma_noise_nodes)\n",
    "\n",
    "#                 # Sampling from Von Mises - Fisher distribution\n",
    "#                 original_coord = original_graph.nodes[original_node][\"coord\"]\n",
    "#                 mean_original = original_coord / np.linalg.norm(original_coord)  # convert to mean vector\n",
    "#                 noisy_coordinate = Sphere().sample(1, distribution='vMF', mu=mean_original,\n",
    "#                                                    kappa=sigma_noise_nodes).sample[0]\n",
    "\n",
    "#                 #noisy_coordinate = noisy_coordinate / np.linalg.norm(noisy_coordinate) * radius\n",
    "#                 noisy_coordinate  = noisy_coordinate * np.linalg.norm(original_coord)\n",
    "#                 noisy_graph.add_node(node_to_add, coord=noisy_coordinate)\n",
    "                \n",
    "#     noisy_coord = list(nx.get_node_attributes(noisy_graph, 'coord').values())\n",
    "                \n",
    "#     # Add Outliers\n",
    "#     if nb_outliers > 0:\n",
    "#         print(\"nb_outliers: \",nb_outliers)\n",
    "#         sphere_random_sampling = generate_sphere_random_sampling(vertex_number=nb_outliers, radius=radius)\n",
    "#         # merge pertubated and outlier coordinates to add edges \n",
    "#         all_coord = noisy_coord + list(sphere_random_sampling)\n",
    "#     else:\n",
    "#         all_coord = noisy_coord\n",
    "\n",
    "#     print(\"nb_outliers: \",nb_outliers)\n",
    "\n",
    "\n",
    "#     compute_noisy_edges = tri_from_hull(all_coord)  # take all peturbated coord and comp conv hull.\n",
    "#     adja = stop.edges_to_adjacency_matrix(compute_noisy_edges)  # compute the new adjacency mat.\n",
    "\n",
    "#     # Extracting the ground-truth correspondence\n",
    "#     ground_truth_permutation = []\n",
    "#     for ar1 in compute_noisy_edges.vertices.view(np.ndarray):\n",
    "#         index = 0\n",
    "#         for ar2 in noisy_coord:\n",
    "#             if np.mean(ar1) == np.mean(ar2):\n",
    "#                 ground_truth_permutation.append(index)\n",
    "#                 index += 1\n",
    "#                 break\n",
    "#             else:\n",
    "#                 index += 1\n",
    "#                 continue\n",
    "\n",
    "#     print(\"Total Ground truth nodes: \", len(ground_truth_permutation))\n",
    "#     print(\"Total number of nodes : \", len(compute_noisy_edges.vertices))\n",
    "\n",
    "#     noisy_graph = nx.from_numpy_matrix(adja.todense())\n",
    "\n",
    "#     node_attribute_dict = {}\n",
    "#     for node in noisy_graph.nodes():\n",
    "#         node_attribute_dict[node] = {\"coord\": np.array(compute_noisy_edges.vertices[node])}\n",
    "\n",
    "#     nx.set_node_attributes(noisy_graph, node_attribute_dict)\n",
    "\n",
    "#     nx.set_edge_attributes(noisy_graph, 1.0, name=\"weight\")\n",
    "\n",
    "#     edge_attribute_dict = {}\n",
    "#     id_counter = 0  # useful for affinity matrix caculation\n",
    "#     for edge in noisy_graph.edges:\n",
    "#         # We calculate the geodesic distance\n",
    "#         end_a = noisy_graph.nodes()[edge[0]][\"coord\"]\n",
    "#         end_b = noisy_graph.nodes()[edge[1]][\"coord\"]\n",
    "#         geodesic_dist = gp.compute_geodesic_distance_sphere(end_a, end_b, radius)\n",
    "\n",
    "#         # add the information in the dictionnary\n",
    "#         edge_attribute_dict[edge] = {\"geodesic_distance\": geodesic_dist, \"id\": id_counter}\n",
    "#         id_counter += 1\n",
    "\n",
    "#     # add the edge attributes to the graph\n",
    "#     nx.set_edge_attributes(noisy_graph, edge_attribute_dict)\n",
    "\n",
    "#     return np.array(ground_truth_permutation), noisy_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_graph_3(original_graph, nb_vertices,nb_outliers,sigma_noise_nodes=1, sigma_noise_edges=1, radius=100):\n",
    "    # Perturbate the coordinates\n",
    "    \n",
    "    noisy_coord = []\n",
    "    key = [] \n",
    "    value = []\n",
    "    \n",
    "    for index in range(nb_vertices):\n",
    "        \n",
    "        # Sampling from Von Mises - Fisher distribution\n",
    "        original_coord = original_graph.nodes[index][\"coord\"]\n",
    "        mean_original = original_coord / np.linalg.norm(original_coord)  # convert to mean vector\n",
    "        noisy_coordinate = Sphere().sample(1, distribution='vMF', mu=mean_original,\n",
    "                                           kappa=sigma_noise_nodes).sample[0]\n",
    "\n",
    "        noisy_coordinate  = noisy_coordinate * np.linalg.norm(original_coord)\n",
    "        #print(noisy_coordinate)\n",
    "        noisy_coord.append(noisy_coordinate)\n",
    "                   \n",
    "    # Add Outliers\n",
    "    sphere_random_sampling = []\n",
    "    if nb_outliers > 0:\n",
    "        print(\"nb_outliers: \",nb_outliers)\n",
    "        sphere_random_sampling = generate_sphere_random_sampling(vertex_number=nb_outliers, radius=radius)\n",
    "        # merge pertubated and outlier coordinates to add edges\n",
    "        all_coord = noisy_coord + list(sphere_random_sampling)\n",
    "    else:\n",
    "        all_coord = noisy_coord\n",
    "\n",
    "    print(\"nb_outliers: \",nb_outliers)\n",
    "\n",
    "    compute_noisy_edges = tri_from_hull(all_coord)  # take all peturbated coord and comp conv hull.\n",
    "    adja = stop.edges_to_adjacency_matrix(compute_noisy_edges)  # compute the new adjacency mat.\n",
    "\n",
    "    # Extracting the ground-truth correspondence\n",
    "\n",
    "    noisy_graph = nx.from_numpy_matrix(adja.todense())\n",
    "    \n",
    "    print(\"all nodes: \",len(noisy_graph.nodes))\n",
    "\n",
    "    node_attribute_dict = {}\n",
    "    for node in noisy_graph.nodes():\n",
    "        node_attribute_dict[node] = {\"coord\": np.array(compute_noisy_edges.vertices[node])}\n",
    "\n",
    "    nx.set_node_attributes(noisy_graph, node_attribute_dict)\n",
    "\n",
    "    nx.set_edge_attributes(noisy_graph, 1.0, name=\"weight\")\n",
    "\n",
    "    edge_attribute_dict = {}\n",
    "    id_counter = 0  # useful for affinity matrix caculation\n",
    "    for edge in noisy_graph.edges:\n",
    "        # We calculate the geodesic distance\n",
    "        end_a = noisy_graph.nodes()[edge[0]][\"coord\"]\n",
    "        end_b = noisy_graph.nodes()[edge[1]][\"coord\"]\n",
    "        geodesic_dist = gp.compute_geodesic_distance_sphere(end_a, end_b, radius)\n",
    "\n",
    "        # add the information in the dictionnary\n",
    "        edge_attribute_dict[edge] = {\"geodesic_distance\": geodesic_dist, \"id\": id_counter}\n",
    "        id_counter += 1\n",
    "\n",
    "    # add the edge attributes to the graph\n",
    "    nx.set_edge_attributes(noisy_graph, edge_attribute_dict)\n",
    "    \n",
    "    ground_truth_permutation = []\n",
    "\n",
    "    for ar1 in noisy_coord:\n",
    "        for i in range(len(noisy_graph.nodes)):\n",
    "            if np.mean(ar1) == np.mean(noisy_graph.nodes[i]['coord']):    \n",
    "                if i >=nb_vertices:\n",
    "                    key.append(i)\n",
    "                    print(\"key\",i)\n",
    "                ground_truth_permutation.append(i)\n",
    "                break\n",
    "                \n",
    "                \n",
    "    for outlier in sphere_random_sampling:\n",
    "        for i in range(len(noisy_graph.nodes)):\n",
    "            if np.mean(noisy_graph.nodes[i]['coord']) == np.mean(outlier):\n",
    "                if i<nb_vertices:\n",
    "                    value.append(i)\n",
    "                    print(\"value\",i)\n",
    "                    \n",
    "    \n",
    "                 \n",
    "    if nb_outliers > 0 and len(key)!=0:\n",
    "        index = 0\n",
    "        for j in range(len(ground_truth_permutation)):\n",
    "            if ground_truth_permutation[j] == key[index]:\n",
    "                ground_truth_permutation[j] = value[index]\n",
    "                index+=1\n",
    "                if index == len(key):\n",
    "                    break\n",
    "                    \n",
    "        key = key + value\n",
    "        value = value + key\n",
    "    \n",
    "        mapping = dict(zip(key,value))\n",
    "        print(mapping)\n",
    "\n",
    "        noisy_graph = nx.relabel_nodes(noisy_graph, mapping) # Relabeling Nodes to keep ground-truth in len(#nodes).\n",
    "\n",
    "    return ground_truth_permutation,noisy_graph,noisy_coord,sphere_random_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 6, 12, 13, 2, 8, 3, 11, 1, 15, 9, 18, 16, 14, 0, 4, 5, 10, 7, 17]"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_gpickle(\"reference_0.gpickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_coord =  list(nx.get_node_attributes(G,'coord').values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy_graph_old_coord = list(nx.get_node_attributes(noisy_graph_old,'coord').values())\n",
    "# noisy_graph_old_coord[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb_outliers:  10\n",
      "nb_outliers:  10\n",
      "all nodes:  30\n",
      "key 22\n",
      "key 26\n",
      "key 28\n",
      "key 24\n",
      "key 23\n",
      "key 29\n",
      "key 27\n",
      "value 13\n",
      "value 8\n",
      "value 3\n",
      "value 15\n",
      "value 9\n",
      "value 0\n",
      "value 5\n",
      "{22: 13, 26: 8, 28: 3, 24: 15, 23: 9, 29: 0, 27: 5, 13: 22, 8: 26, 3: 28, 15: 24, 9: 23, 0: 29, 5: 27}\n"
     ]
    }
   ],
   "source": [
    "ground_truth, noisy_graph,noisy_coord,outliers = generate_noisy_graph_3(G,len(original_coord),10,10,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeView((29, 1, 2, 28, 4, 27, 6, 7, 26, 23, 10, 11, 12, 22, 14, 24, 16, 17, 18, 19, 20, 21, 13, 9, 15, 25, 8, 5, 3, 0))"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 6, 12, 13, 2, 8, 3, 11, 1, 15, 9, 18, 16, 14, 0, 4, 5, 10, 7, 17]"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    match:  [ 97.0114961   10.72561958 -21.76535573] [ 97.0114961   10.72561958 -21.76535573]\n",
      "    match:  [-16.50730932 -95.82102031 -23.36323622] [-16.50730932 -95.82102031 -23.36323622]\n",
      "    match:  [-48.9121704   74.06926908 -46.05803908] [-48.9121704   74.06926908 -46.05803908]\n",
      "    match:  [-36.76963146   0.8223222  -92.99095649] [-36.76963146   0.8223222  -92.99095649]\n",
      "    match:  [70.02354486 56.33102381 43.85794023] [70.02354486 56.33102381 43.85794023]\n",
      "    match:  [-46.65995061 -79.1154977  -39.54221835] [-46.65995061 -79.1154977  -39.54221835]\n",
      "    match:  [-12.96400909 -47.27044067  87.16329449] [-12.96400909 -47.27044067  87.16329449]\n",
      "    match:  [-54.27250579  71.83164411  43.52826692] [-54.27250579  71.83164411  43.52826692]\n",
      "    match:  [-42.90835419 -87.57768752  22.11383705] [-42.90835419 -87.57768752  22.11383705]\n",
      "    match:  [ 35.28525505 -79.17178789 -49.86761252] [ 35.28525505 -79.17178789 -49.86761252]\n",
      "    match:  [ 15.13235383 -64.28237478  75.09186481] [ 15.13235383 -64.28237478  75.09186481]\n",
      "    match:  [ 20.68717223 -13.10998749 -96.95446938] [ 20.68717223 -13.10998749 -96.95446938]\n",
      "    match:  [28.54118811 95.63336305 -6.29765461] [28.54118811 95.63336305 -6.29765461]\n",
      "    match:  [ 95.69680276   6.5209186  -28.27719157] [ 95.69680276   6.5209186  -28.27719157]\n",
      "    match:  [ 65.8541618   64.81193227 -38.24451345] [ 65.8541618   64.81193227 -38.24451345]\n",
      "    match:  [15.16190267  9.94154108 98.34267877] [15.16190267  9.94154108 98.34267877]\n",
      "    match:  [41.50583018 83.36636401 36.43234021] [41.50583018 83.36636401 36.43234021]\n",
      "    match:  [-45.16581274 -22.26481637  86.39633853] [-45.16581274 -22.26481637  86.39633853]\n",
      "    match:  [-26.32898739 -70.18262424  66.19051048] [-26.32898739 -70.18262424  66.19051048]\n",
      "    match:  [ 1.70742464e-02 -9.73760466e+01  2.27575318e+01] [ 1.70742464e-02 -9.73760466e+01  2.27575318e+01]\n",
      "Total indexes 20\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "for i in ground_truth:\n",
    "    if np.mean(noisy_graph.nodes[i]['coord'] == noisy_coord[index]):\n",
    "        print(\"    match: \",noisy_graph.nodes[i]['coord'],noisy_coord[index])\n",
    "    else:\n",
    "        print(\"Not match: \",noisy_graph.nodes[i]['coord'],noisy_coord[index])\n",
    "    index+=1\n",
    "print(\"Total indexes\", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 36.19014513, -60.81998733, -70.64844327],\n",
       "       [ -8.06574392, -92.24560988, -37.75832667],\n",
       "       [ 26.49864801, -96.05052698,  -8.4922271 ],\n",
       "       [ 18.98045238,  44.42569073, -87.55627008],\n",
       "       [-71.11433319,  51.9061427 , -47.41839269],\n",
       "       [ 41.21954146,  17.16894909,  89.47724062],\n",
       "       [-72.11741004,  32.65028254, -61.09859424],\n",
       "       [  7.13361937, -98.64339734,  14.78484482],\n",
       "       [-81.31148116,  33.30849211, -47.73874092],\n",
       "       [-48.21938621, -70.98311831,  51.34479242]])"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "25\n",
      "26\n",
      "28\n",
      "24\n",
      "23\n",
      "21\n",
      "29\n",
      "20\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "outlier_index = []\n",
    "for outlier in outliers:\n",
    "    for i in range(len(noisy_graph.nodes)):\n",
    "        if np.mean(noisy_graph.nodes[i]['coord']) == np.mean(outlier):\n",
    "            print(i)\n",
    "            outlier_index.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(outlier_index).intersection(ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_noisy_edges = tri_from_hull(noisy_coord_3+list(outlier_coords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_sequence = []\n",
    "\n",
    "coords_new = compute_noisy_edges.vertices.view(np.ndarray)\n",
    "\n",
    "key = []\n",
    "value = []\n",
    "\n",
    "for ar1 in noisy_coord_3:\n",
    "    for i in range(len(coords_new)):\n",
    "        if np.mean(ar1) == np.mean(coords_new[i]):\n",
    "            if i >=20:\n",
    "                key.append(i)\n",
    "                print(\"key:\",i)\n",
    "            ground_truth_sequence.append(i)\n",
    "            \n",
    "            \n",
    "for outlier in outlier_coords:\n",
    "    for i in range(len(coords_new)):\n",
    "        if np.mean(coords_new[i]) == np.mean(outlier):\n",
    "            if i<20:\n",
    "                value.append(i)\n",
    "                print(\"value:\",i)\n",
    "                \n",
    "index = 0\n",
    "for j in range(len(ground_truth_sequence)):\n",
    "    if ground_truth_sequence[j] == key[index]:\n",
    "        ground_truth_sequence[j] = value[index]\n",
    "        index+=1\n",
    "                \n",
    "mapping = dict(zip(key,value))\n",
    "\n",
    "noisy_graph_3 = nx.relabel_nodes(noisy_graph_3, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_coord_3[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_graph_3.nodes[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_G.nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_G = noisy_graph_3.copy()\n",
    "\n",
    "test_G = nx.relabel_nodes(test_G, mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

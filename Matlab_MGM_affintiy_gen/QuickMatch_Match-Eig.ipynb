{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/rohit/PhD_Work/GM_my_version/Graph_matching/\")\n",
    "from sklearn.cluster import KMeans\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from graph_matching_tools.io.graph_dataset import GraphDataset\n",
    "from graph_matching_tools.metrics import matching\n",
    "import matplotlib.pyplot as plt\n",
    "from graph_matching_tools.algorithms.multiway.hippi import hippi_multiway_matching\n",
    "from graph_matching_tools.algorithms.kernels.gaussian import create_gaussian_node_kernel\n",
    "from graph_matching_tools.algorithms.kernels.utils import create_full_node_affinity_matrix\n",
    "from graph_matching_tools.algorithms.multiway.stiefel import sparse_stiefel_manifold_sync\n",
    "from graph_matching_tools.algorithms.multiway import quickmatch\n",
    "from graph_matching_tools.algorithms.multiway import matcheig\n",
    "import tools.graph_processing as gp\n",
    "import scipy.io as sio\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_permutation_matrix_from_dictionary(matching, g_sizes):\n",
    "    \"\"\"\n",
    "    Create the full permutation matrix from the matching result\n",
    "    :param matching: the matching result for each graph (nodes number, assignment)\n",
    "    :param g_sizes: the list of the size of the different graph\n",
    "    :return: the full permutation matrix\n",
    "    \"\"\"\n",
    "    f_size = int(np.sum(g_sizes))\n",
    "    res = np.zeros((f_size, f_size))\n",
    "\n",
    "    idx1 = 0\n",
    "    for i_g1 in range(len(g_sizes)):\n",
    "        idx2 = 0\n",
    "        for i_g2 in range(len(g_sizes)):\n",
    "            match = matching[\"{},{}\".format(i_g1, i_g2)]\n",
    "            for k in match:\n",
    "                res[idx1 + int(k), idx2 + match[k]] = 1\n",
    "            idx2 += g_sizes[i_g2]\n",
    "        idx1 += g_sizes[i_g1]\n",
    "        \n",
    "    np.fill_diagonal(res,1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dummy_rowcol_X(bulk_X, graphs):\n",
    "    \n",
    "    dummy_mask = [list(nx.get_node_attributes(graph,'is_dummy').values()) for graph in graphs]\n",
    "    dummy_mask = sum(dummy_mask,[])\n",
    "    dummy_indexes = [i for i in range(len(dummy_mask)) if dummy_mask[i]==True]     \n",
    "    \n",
    "    bulk_X = np.delete(bulk_X,dummy_indexes,0) # delete the dummy rows\n",
    "    bulk_X = np.delete(bulk_X,dummy_indexes,1) # delete the dummy columns\n",
    "    \n",
    "    return bulk_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_graph_folder = '../data/simu_graph/NEW_SIMUS_JULY_11/'\n",
    "path_to_dummy_graphs_folder = '../data/simu_graph/NEW_SIMUS_JULY_11_with_dummy/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial:  0\n",
      "Noise folder:  noise_100,outliers_varied\n",
      "X_KerGM shape (13563, 13563)\n",
      "P shape (13563, 13563)\n",
      "saved mat file ..\n",
      "res shape (12085, 12085)\n",
      "X shape (12085, 12085)\n",
      "--- 407.6538908481598 seconds ---\n",
      "F1:  0.44437857252251584\n",
      "Noise folder:  noise_400,outliers_varied\n",
      "X_KerGM shape (13700, 13700)\n",
      "P shape (13700, 13700)\n",
      "saved mat file ..\n",
      "res shape (12008, 12008)\n",
      "X shape (12008, 12008)\n",
      "--- 577.7259650230408 seconds ---\n",
      "F1:  0.7675501007312527\n",
      "Noise folder:  noise_200,outliers_varied\n",
      "X_KerGM shape (14111, 14111)\n"
     ]
    }
   ],
   "source": [
    "trials = np.sort(os.listdir(path_to_graph_folder))\n",
    "\n",
    "sigma = 200\n",
    "\n",
    "udim = 101\n",
    "\n",
    "scores = {100:[],200:[],400:[],1000:[]}\n",
    "precision = {100:[],200:[],400:[],1000:[]}\n",
    "recall = {100:[],200:[],400:[],1000:[]}\n",
    "\n",
    "for trial in trials:\n",
    "    \n",
    "    if float(trial) >= 0.0:\n",
    "        \n",
    "        print('trial: ', trial)\n",
    "\n",
    "        all_files = os.listdir(path_to_graph_folder+trial)\n",
    "\n",
    "        for folder in all_files:\n",
    "\n",
    "            if os.path.isdir(path_to_graph_folder+trial+'/'+ folder):\n",
    "\n",
    "                print('Noise folder: ',folder)\n",
    "                \n",
    "                path_to_graphs = path_to_graph_folder + '/' + trial + '/' + folder+'/graphs/'\n",
    "                path_to_dummy_graphs = path_to_dummy_graphs_folder + '/' + trial + '/' + folder+'/0/graphs/'\n",
    "                path_to_groundtruth_ref = path_to_graph_folder + '/' + trial +'/' + folder + '/permutation_to_ref_graph.gpickle'\n",
    "                path_to_groundtruth  = path_to_graph_folder + '/' + trial + '/' + folder + '/ground_truth.gpickle'\n",
    "                \n",
    "            \n",
    "                noise = folder.split(',')[0].split('_')[1]\n",
    "#                 graph_meta = GraphDataset(path_to_graphs, path_to_groundtruth_ref)\n",
    "                ground_truth =  nx.read_gpickle(path_to_groundtruth)\n",
    "    \n",
    "                list_graphs_dummy = gp.load_graphs_in_list(path_to_dummy_graphs)\n",
    "                sizes_dummy = [nx.number_of_nodes(g) for g in list_graphs_dummy]\n",
    "        \n",
    "                list_graphs = gp.load_graphs_in_list(path_to_graphs)\n",
    "                sizes = [nx.number_of_nodes(g) for g in list_graphs]\n",
    "\n",
    "                res = get_permutation_matrix_from_dictionary(ground_truth, sizes)\n",
    "                \n",
    "#                 node_kernel = create_gaussian_node_kernel(sigma,'coord')\n",
    "#                 knode = create_full_node_affinity_matrix(list_graphs, node_kernel)\n",
    "                start_time = time.time()\n",
    "    \n",
    "                # Initialize using pairwise results\n",
    "                path_to_X_mat = path_to_graph_folder + '/' + trial + '/' + folder\n",
    "                path_X_kergm = path_to_X_mat + '/X_pairwise_kergm.mat' \n",
    "                knode = sio.loadmat(path_X_kergm)['full_assignment_mat']\n",
    "                print('X_KerGM shape', knode.shape)\n",
    "                \n",
    "                P = matcheig.matcheig(knode, udim, sizes_dummy)\n",
    "#                 P = U @ U.T\n",
    "\n",
    "#                 U = quickmatch.quickmatch(graph_meta.list_graphs, 'coord', 0.26, 0.63)\n",
    "    \n",
    "#                 P = U @ U.T\n",
    "\n",
    "                X_MatchEig = {}\n",
    "                X_MatchEig['X'] = P\n",
    "                \n",
    "                print('P shape', P.shape)\n",
    "                \n",
    "                #save with dummy\n",
    "                #sio.savemat(path_to_graph_folder + '/' + trial + '/' + folder + '/X_MatchEig.mat',X_MatchEig)\n",
    "                print('saved mat file ..')\n",
    "                \n",
    "                #remove dummy to compute F1\n",
    "                X = remove_dummy_rowcol_X(P, list_graphs_dummy)\n",
    "                print('res shape', res.shape)\n",
    "                print('X shape', X.shape)\n",
    "                \n",
    "                print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "                \n",
    "                f1, prec, rec = matching.compute_f1score(X,res)\n",
    "                print('F1: ', f1)\n",
    "\n",
    "                scores[int(noise)].append(f1)\n",
    "                precision[int(noise)].append(prec)\n",
    "                recall[int(noise)].append(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( precision, open( \"precision_match_eig.pickle\", \"wb\" ) )\n",
    "pickle.dump( scores, open( \"scores_match_eig.pickle\", \"wb\" ) )\n",
    "pickle.dump( recall, open( \"recall_match_eig.pickle\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = pickle.load( open( \"recall_match_eig.pickle\", \"rb\" ) )\n",
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores\n",
    "match_eig = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = {100: [0.633630289532294,\n",
    "  0.5964303420922161,\n",
    "  0.6086483546369322,\n",
    "  0.60486674391657],\n",
    " 400: [0.7458039406470445,\n",
    "  0.760277365032194,\n",
    "  0.6800560485754321,\n",
    "  0.6219205630970336],\n",
    " 700: [0.7498272287491362,\n",
    "  0.7080394922425952,\n",
    "  0.61010101010101,\n",
    "  0.6971312515866971],\n",
    " 1000: [0.7381974248927039,\n",
    "  0.681999386691199,\n",
    "  0.6434782608695652,\n",
    "  0.6336032388663968],\n",
    " 1300: [0.7844391153743672,\n",
    "  0.7408305547135495,\n",
    "  0.6871002132196162,\n",
    "  0.7225617208506478]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_match = {100: [0.14484272128749087,\n",
    "  0.12236842105263158,\n",
    "  0.1318051575931232,\n",
    "  0.1385714285714286],\n",
    " 400: [0.13870776526378187,\n",
    "  0.13641755634638197,\n",
    "  0.13211009174311927,\n",
    "  0.15546218487394958],\n",
    " 700: [0.16313213703099513,\n",
    "  0.15613910574875797,\n",
    "  0.15036496350364964,\n",
    "  0.14882506527415146],\n",
    " 1000: [0.14947498455836938,\n",
    "  0.15445859872611464,\n",
    "  0.1475529583637692,\n",
    "  0.15547703180212016],\n",
    " 1300: [0.1452513966480447,\n",
    "  0.15882352941176472,\n",
    "  0.14603616133518776,\n",
    "  0.13750767341927564]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_mean_std(scores):\n",
    "    \n",
    "    avg_scores = []\n",
    "    std_scores = []\n",
    "\n",
    "    for keys,values in scores.items():\n",
    "        avg_scores.append(np.mean(values))\n",
    "        std_scores.append(np.std(values))\n",
    "        \n",
    "    return np.array(avg_scores), np.array(std_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_mean, kmeans_std  = score_mean_std(kmeans)\n",
    "quick_means, quick_std  = score_mean_std(quick_match)\n",
    "match_mean, match_std = score_mean_std(match_eig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.plot(list(scores.keys()), kmeans_mean ,label = 'kmeans')\n",
    "plt.fill_between(list(scores.keys()), kmeans_mean - kmeans_std, kmeans_mean + kmeans_std, alpha=0.2)\n",
    "\n",
    "\n",
    "plt.plot(list(scores.keys()), quick_means ,label = 'quick_match')\n",
    "plt.fill_between(list(scores.keys()), quick_means - quick_std, quick_means + quick_std, alpha=0.2)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(list(scores.keys()), match_mean ,label = 'match_eig')\n",
    "plt.fill_between(list(scores.keys()), match_mean - match_std, match_mean + match_std, alpha=0.2)\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('kappa',fontweight=\"bold\")\n",
    "plt.ylabel('F1 score',fontweight=\"bold\")\n",
    "plt.legend(loc = 'lower left')\n",
    "plt.title('kmeans, quickmatch and matcheig on simultion for different kappa values',fontweight=\"bold\")\n",
    "plt.gca().yaxis.grid(True)\n",
    "plt.gca().invert_xaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_path = '/home/rohit/PhD_Work/GM_my_version/Graph_matching/data/simu_graph/simu_test_single_noise/0.0/noise_1000,outliers_varied/ground_truth.gpickle'\n",
    "dummy_path = '/home/rohit/PhD_Work/GM_my_version/Graph_matching/data/simu_graph/simu_with_dummy/0.0/noise_1000,outliers_varied/0/graphs/'\n",
    "non_dummy_path = '/home/rohit/PhD_Work/GM_my_version/Graph_matching/data/simu_graph/simu_test_single_noise/0.0/noise_1000,outliers_varied/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cao_kappa_1000 = sio.loadmat('X_cao_cst_o.mat')['X']\n",
    "mals_kappa_1000 = sio.loadmat('X_mALS.mat')['X']\n",
    "all_dummy_graphs = [nx.read_gpickle(dummy_path+'/'+g) for g in np.sort(os.listdir(dummy_path))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_m = GraphDataset(non_dummy_path + '/graphs', non_dummy_path + '/permutation_to_ref_graph.gpickle')\n",
    "gt = nx.read_gpickle(gt_path)\n",
    "res = get_permutation_matrix_from_dictionary(gt, graph_m.sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_mask = [list(nx.get_node_attributes(graph,'is_dummy').values()) for graph in all_dummy_graphs]\n",
    "dummy_mask = sum(dummy_mask,[])\n",
    "dummy_indexes = [i for i in range(len(dummy_mask)) if dummy_mask[i]==True]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cao = np.delete(cao_kappa_1000,dummy_indexes,0) # delete the dummy rows\n",
    "cao = np.delete(cao,dummy_indexes,1) # delete the dummy columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mAls = np.delete(mals_kappa_1000,dummy_indexes,0) # delete the dummy rows\n",
    "mAls = np.delete(mAls,dummy_indexes,1) # delete the dummy columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matching.compute_f1score(cao,res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matching.compute_f1score(mAls,res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

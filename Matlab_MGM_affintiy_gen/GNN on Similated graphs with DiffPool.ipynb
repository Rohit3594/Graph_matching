{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e07b2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f4b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import os.path as osp\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch_geometric.nn import DenseSAGEConv, dense_diff_pool,dense_mincut_pool\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/rohit/PhD_Work/GM_my_version/Graph_matching/\")\n",
    "from sklearn.cluster import KMeans\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from graph_generation.load_graphs_and_create_metadata import dataset_metadata\n",
    "from graph_matching_tools.metrics import matching\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sco\n",
    "import slam.io as sio\n",
    "from scipy.special import softmax\n",
    "import pickle\n",
    "from scipy.stats import betabinom\n",
    "import seaborn as sns\n",
    "import tools.graph_processing as gp\n",
    "import tools.graph_visu as gv\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "import torch\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from math import ceil\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import TopKPooling\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch_geometric.utils import to_dense_batch, to_dense_adj\n",
    "from torch_geometric.nn import GCNConv, DenseGraphConv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fbe335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_trials = '..//data/simu_graph/simu_genders//'\n",
    "path_sub_dir = '/noise_100,outliers_varied/graphs/'\n",
    "all_trials = np.sort(list(map(int,os.listdir(path_trials)))) # all trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f7134e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_graphs_same_trial = []\n",
    "all_graphs_labels_same_trial = []\n",
    "\n",
    "referene_graphs = []\n",
    "\n",
    "for trial in all_trials:\n",
    "    \n",
    "    #referene_graphs.append(nx.read_gpickle(path_trials + str(trial) + '/reference_'+ str(trial) +'.gpickle'))\n",
    "    \n",
    "    trial_folder = path_trials + str(trial) + path_sub_dir\n",
    "    all_graphs_same_trial.append(gp.load_graphs_in_list(trial_folder)) # append graphs\n",
    "    all_graphs_labels_same_trial.append(np.ones([len(gp.load_graphs_in_list(trial_folder))])*trial) # append corr lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17540b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking 300 graph from each population for learning \n",
    "\n",
    "simu_train_set = []\n",
    "simu_train_labels = []\n",
    "\n",
    "for graph_set,graph_labels in zip(all_graphs_same_trial,all_graphs_labels_same_trial):\n",
    "    simu_train_set.extend(graph_set[:300])\n",
    "    simu_train_labels.extend(graph_labels[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6221b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0614172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_remove_dummy_nodes(graph):\n",
    "    nodes_dummy_true = [x for x,y in graph.nodes(data=True) if y['is_dummy']==True]\n",
    "    graph.remove_nodes_from(nodes_dummy_true)\n",
    "    #print(len(graph.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82bebfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7111/1758284362.py:15: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  y = torch.tensor(simu_train_labels[i],dtype=torch.long) # add graph label\n"
     ]
    }
   ],
   "source": [
    "# Convert networkx graphs to pyg graphs\n",
    "\n",
    "sulcal_simu_dataset = []\n",
    "\n",
    "for i,g in enumerate(simu_train_set):\n",
    "    #graph_remove_dummy_nodes(g) # remove dummy nodes\n",
    "    g.remove_edges_from(nx.selfloop_edges(g)) # remove self loop edges\n",
    "    \n",
    "    attr_coords = np.array(list(nx.get_node_attributes(g,'coord').values())) #simu attribute (coords)\n",
    "    \n",
    "    x = torch.tensor(attr_coords,dtype=torch.float)\n",
    "    \n",
    "    #adj = torch.tensor(nx.adjacency_matrix(g).todense(),dtype=torch.float)\n",
    "    \n",
    "    y = torch.tensor(simu_train_labels[i],dtype=torch.long) # add graph label\n",
    "    edge_index = torch.tensor(list(g.edges))\n",
    "    \n",
    "    sulcal_simu_dataset.append(Data(x=x, y=y, edge_index=edge_index.t().contiguous())) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fabb40a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sulcal_simu_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc3a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66127abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(sulcal_simu_dataset)\n",
    "\n",
    "train_dataset = sulcal_simu_dataset[:len(sulcal_simu_dataset)-500]\n",
    "test_dataset = sulcal_simu_dataset[len(sulcal_simu_dataset)-500:]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # dense data loader for using adjacency\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687d759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8186f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_node_features = train_dataset[0].num_features\n",
    "num_node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47de53f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_nodes = 150\n",
    "# class MyFilter(object):\n",
    "#     def __call__(self, data):\n",
    "#         return data.num_nodes <= max_nodes\n",
    "\n",
    "\n",
    "# path = osp.join(osp.dirname(osp.realpath('./tmp')), '..', 'data',\n",
    "#                 'PROTEINS_dense')\n",
    "# dataset = TUDataset(path, name='PROTEINS', transform=T.ToDense(max_nodes),\n",
    "#                     pre_filter=MyFilter())\n",
    "# dataset = dataset.shuffle()\n",
    "# n = (len(dataset) + 9) // 10\n",
    "# test_dataset = dataset[:n]\n",
    "# val_dataset = dataset[n:2 * n]\n",
    "# train_dataset = dataset[2 * n:]\n",
    "# test_loader = DenseDataLoader(test_dataset, batch_size=20)\n",
    "# val_loader = DenseDataLoader(val_dataset, batch_size=20)\n",
    "# train_loader = DenseDataLoader(train_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bbd216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b424078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GNN(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, hidden_channels, out_channels,\n",
    "#                  normalize=False, lin=True):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.conv1 = DenseSAGEConv(in_channels, hidden_channels, normalize)\n",
    "#         self.bn1 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "#         self.conv2 = DenseSAGEConv(hidden_channels, hidden_channels, normalize)\n",
    "#         self.bn2 = torch.nn.BatchNorm1d(hidden_channels)\n",
    "#         self.conv3 = DenseSAGEConv(hidden_channels, out_channels, normalize)\n",
    "#         self.bn3 = torch.nn.BatchNorm1d(out_channels)\n",
    "\n",
    "#         if lin is True:\n",
    "#             self.lin = torch.nn.Linear(2 * hidden_channels + out_channels,\n",
    "#                                        out_channels)\n",
    "#         else:\n",
    "#             self.lin = None\n",
    "\n",
    "#     def bn(self, i, x):\n",
    "#         batch_size, num_nodes, num_channels = x.size()\n",
    "\n",
    "#         x = x.view(-1, num_channels)\n",
    "#         x = getattr(self, f'bn{i}')(x)\n",
    "#         x = x.view(batch_size, num_nodes, num_channels)\n",
    "#         return x\n",
    "\n",
    "#     def forward(self, x, adj, mask=None):\n",
    "#         batch_size, num_nodes, in_channels = x.size()\n",
    "\n",
    "#         x0 = x\n",
    "#         x1 = self.bn(1, self.conv1(x0, adj, mask).relu())\n",
    "#         x2 = self.bn(2, self.conv2(x1, adj, mask).relu())\n",
    "#         x3 = self.bn(3, self.conv3(x2, adj, mask).relu())\n",
    "\n",
    "#         x = torch.cat([x1, x2, x3], dim=-1)\n",
    "\n",
    "#         if self.lin is not None:\n",
    "#             x = self.lin(x).relu()\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "\n",
    "#         num_nodes = ceil(0.25 * max_nodes)\n",
    "#         self.gnn1_pool = GNN(dataset.num_features, 64, num_nodes)\n",
    "#         self.gnn1_embed = GNN(dataset.num_features, 64, 64, lin=False)\n",
    "\n",
    "#         num_nodes = ceil(0.25 * num_nodes)\n",
    "#         self.gnn2_pool = GNN(3 * 64, 64, num_nodes)\n",
    "#         self.gnn2_embed = GNN(3 * 64, 64, 64, lin=False)\n",
    "\n",
    "#         self.gnn3_embed = GNN(3 * 64, 64, 64, lin=False)\n",
    "\n",
    "#         self.lin1 = torch.nn.Linear(3 * 64, 64)\n",
    "#         self.lin2 = torch.nn.Linear(64, dataset.num_classes)\n",
    "\n",
    "#     def forward(self, x, adj, mask=None):\n",
    "#         s = self.gnn1_pool(x, adj, mask)\n",
    "#         x = self.gnn1_embed(x, adj, mask)\n",
    "\n",
    "#         x, adj, l1, e1 = dense_diff_pool(x, adj, s, mask)\n",
    "\n",
    "#         s = self.gnn2_pool(x, adj)\n",
    "#         x = self.gnn2_embed(x, adj)\n",
    "\n",
    "#         x, adj, l2, e2 = dense_diff_pool(x, adj, s)\n",
    "\n",
    "#         x = self.gnn3_embed(x, adj)\n",
    "\n",
    "#         x = x.mean(dim=1)\n",
    "#         x = self.lin1(x).relu()\n",
    "#         x = self.lin2(x)\n",
    "#         return F.log_softmax(x, dim=-1), l1 + l2, e1 + e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ac9b338",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class mincutnet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels=16):\n",
    "        super(mincutnet, self).__init__()\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        num_of_centers =  20\n",
    "        self.pool1 = Linear(hidden_channels, num_of_centers) # The degree of the node belonging to any of the centers\n",
    "        self.conv2 = DenseGraphConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        #self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch): # x torch.Size([661, 3]),  data.batch  torch.Size([783])\n",
    "        x = F.relu(self.conv1(x, edge_index))  #x torch.Size([661, 32])\n",
    "        #print('x :',x.shape)\n",
    "        x, mask = to_dense_batch(x, batch) #now x torch.Size([1, 661, 32]) ; mask torch.Size([20, 122])\n",
    "        #print('mask :',mask.shape)\n",
    "        \n",
    "        adj = to_dense_adj(edge_index, batch) # adj torch.Size([1, 661, 661])\n",
    "        s = self.pool1(x) # s torch.Size([1, 661, 20])\n",
    "        #print('s :',s.shape)\n",
    "        #print('adj :',adj.shape)\n",
    "        \n",
    "        x, adj, mincut_loss, ortho_loss = dense_mincut_pool(x, adj, s, mask) # x torch.Size([1, 20, 32]),  adj torch.Size([1, 20, 20])\n",
    "        x = self.conv2(x, adj) #x torch.Size([1, 20, 32])\n",
    "        g_emb = x.mean(dim=1) # x torch.Size([1, 32])\n",
    "        #x = F.relu(self.lin1(x)) # x torch.Size([1, 32])\n",
    "        out = self.lin2(g_emb) #x torch.Size([1, 2])\n",
    "        return out, g_emb, mincut_loss, ortho_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c396eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Net(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, hidden_channels=32):\n",
    "#         super(Net, self).__init__()\n",
    "        \n",
    "#         self.in_head = 2\n",
    "#         self.out_head = 1\n",
    "#         self.emb_dim = 16\n",
    "        \n",
    "#         max_nodes = 101\n",
    "        \n",
    "\n",
    "#         self.conv1 = GATConv(in_channels, hidden_channels, heads=self.in_head)\n",
    "#         self.conv2 = GATConv(hidden_channels*self.in_head, hidden_channels,concat=False)\n",
    "#         self.conv3 = GATConv(hidden_channels, self.emb_dim, concat=False, dropout=0.6)\n",
    "        \n",
    "#         num_nodes = ceil(0.20 * max_nodes)\n",
    "#         self.pool1 = Linear(in_channels, num_nodes)\n",
    "        \n",
    "#         self.conv4 = GCNConv(self.emb_dim,  self.emb_dim)\n",
    "        \n",
    "#         self.lin = Linear(self.emb_dim, out_channels)\n",
    "\n",
    "#     def forward(self, x, edge_index, batch): \n",
    "        \n",
    "#         s = self.pool1(x)\n",
    "        \n",
    "#         print('s :',s.shape)\n",
    "\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = x.relu()\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         x = x.relu()\n",
    "#         emb,attn_weights = self.conv3(x, edge_index,return_attention_weights=True)\n",
    "        \n",
    "#         print('emb :',emb.shape)\n",
    "         \n",
    "#         x, mask = to_dense_batch(emb, batch) \n",
    "#         adj = to_dense_adj(edge_index, batch)\n",
    "        \n",
    "#         print('adj :',adj.shape)\n",
    "#         print('mask :',mask.shape)\n",
    "        \n",
    "         \n",
    "#         x, adj,_ ,_ = dense_diff_pool(x, adj, s, mask) \n",
    "        \n",
    "#         x = self.conv4(x, adj) \n",
    "#         g_emb = x.mean(dim=1)\n",
    "        \n",
    "#         out = self.lin(g_emb) #x torch.Size([1, 2])\n",
    "#         return out, emb, g_emb, attn_weights\n",
    "    \n",
    "# model = Net(num_node_features, len(all_trials))\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a85c8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mincutnet(\n",
      "  (conv1): GCNConv(3, 16)\n",
      "  (pool1): Linear(in_features=16, out_features=20, bias=True)\n",
      "  (conv2): DenseGraphConv(16, 16)\n",
      "  (lin2): Linear(in_features=16, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = mincutnet(num_node_features, len(all_trials))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b0ff946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = GCN(hidden_channels=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    for data in train_loader:# Iterate in batches over the training dataset.\n",
    "        out, g_emb, _,_= model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        out, g_emb, _,_ =  model(data.x, data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        \n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b052d299",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.3780, Test Acc: 0.3560\n",
      "Epoch: 002, Train Acc: 0.6172, Test Acc: 0.6280\n",
      "Epoch: 003, Train Acc: 0.6972, Test Acc: 0.6820\n",
      "Epoch: 004, Train Acc: 0.6436, Test Acc: 0.6420\n",
      "Epoch: 005, Train Acc: 0.7992, Test Acc: 0.8020\n",
      "Epoch: 006, Train Acc: 0.5660, Test Acc: 0.5680\n",
      "Epoch: 007, Train Acc: 0.5592, Test Acc: 0.5700\n",
      "Epoch: 008, Train Acc: 0.8080, Test Acc: 0.8140\n",
      "Epoch: 009, Train Acc: 0.8264, Test Acc: 0.8100\n",
      "Epoch: 010, Train Acc: 0.8156, Test Acc: 0.8000\n",
      "Epoch: 011, Train Acc: 0.8528, Test Acc: 0.8560\n",
      "Epoch: 012, Train Acc: 0.6996, Test Acc: 0.6960\n",
      "Epoch: 013, Train Acc: 0.7372, Test Acc: 0.7420\n",
      "Epoch: 014, Train Acc: 0.8488, Test Acc: 0.8200\n",
      "Epoch: 015, Train Acc: 0.8448, Test Acc: 0.8500\n",
      "Epoch: 016, Train Acc: 0.8688, Test Acc: 0.8780\n",
      "Epoch: 017, Train Acc: 0.8456, Test Acc: 0.8340\n",
      "Epoch: 018, Train Acc: 0.7836, Test Acc: 0.7780\n",
      "Epoch: 019, Train Acc: 0.7536, Test Acc: 0.7680\n",
      "Epoch: 020, Train Acc: 0.8028, Test Acc: 0.8040\n",
      "Epoch: 021, Train Acc: 0.8448, Test Acc: 0.8140\n",
      "Epoch: 022, Train Acc: 0.8448, Test Acc: 0.8280\n",
      "Epoch: 023, Train Acc: 0.8576, Test Acc: 0.8600\n",
      "Epoch: 024, Train Acc: 0.8644, Test Acc: 0.8740\n",
      "Epoch: 025, Train Acc: 0.8296, Test Acc: 0.7980\n",
      "Epoch: 026, Train Acc: 0.8584, Test Acc: 0.8500\n",
      "Epoch: 027, Train Acc: 0.8400, Test Acc: 0.8500\n",
      "Epoch: 028, Train Acc: 0.9016, Test Acc: 0.8820\n",
      "Epoch: 029, Train Acc: 0.8480, Test Acc: 0.8360\n",
      "Epoch: 030, Train Acc: 0.7396, Test Acc: 0.7160\n",
      "Epoch: 031, Train Acc: 0.8920, Test Acc: 0.9000\n",
      "Epoch: 032, Train Acc: 0.8092, Test Acc: 0.7800\n",
      "Epoch: 033, Train Acc: 0.8436, Test Acc: 0.7960\n",
      "Epoch: 034, Train Acc: 0.8940, Test Acc: 0.8900\n",
      "Epoch: 035, Train Acc: 0.9132, Test Acc: 0.9020\n",
      "Epoch: 036, Train Acc: 0.8788, Test Acc: 0.9060\n",
      "Epoch: 037, Train Acc: 0.7816, Test Acc: 0.7820\n",
      "Epoch: 038, Train Acc: 0.6060, Test Acc: 0.6200\n",
      "Epoch: 039, Train Acc: 0.8520, Test Acc: 0.8580\n",
      "Epoch: 040, Train Acc: 0.9100, Test Acc: 0.9100\n",
      "Epoch: 041, Train Acc: 0.8700, Test Acc: 0.8500\n",
      "Epoch: 042, Train Acc: 0.9052, Test Acc: 0.9020\n",
      "Epoch: 043, Train Acc: 0.8476, Test Acc: 0.8560\n",
      "Epoch: 044, Train Acc: 0.8532, Test Acc: 0.8420\n",
      "Epoch: 045, Train Acc: 0.8572, Test Acc: 0.8820\n",
      "Epoch: 046, Train Acc: 0.8016, Test Acc: 0.7720\n",
      "Epoch: 047, Train Acc: 0.8096, Test Acc: 0.7800\n",
      "Epoch: 048, Train Acc: 0.8516, Test Acc: 0.8200\n",
      "Epoch: 049, Train Acc: 0.8668, Test Acc: 0.8660\n",
      "Epoch: 050, Train Acc: 0.8588, Test Acc: 0.8600\n",
      "Epoch: 051, Train Acc: 0.8676, Test Acc: 0.8320\n",
      "Epoch: 052, Train Acc: 0.8588, Test Acc: 0.8540\n",
      "Epoch: 053, Train Acc: 0.9016, Test Acc: 0.8920\n",
      "Epoch: 054, Train Acc: 0.9144, Test Acc: 0.8900\n",
      "Epoch: 055, Train Acc: 0.8796, Test Acc: 0.8820\n",
      "Epoch: 056, Train Acc: 0.8968, Test Acc: 0.8680\n",
      "Epoch: 057, Train Acc: 0.8524, Test Acc: 0.8500\n",
      "Epoch: 058, Train Acc: 0.8620, Test Acc: 0.8540\n",
      "Epoch: 059, Train Acc: 0.8732, Test Acc: 0.8500\n",
      "Epoch: 060, Train Acc: 0.9044, Test Acc: 0.8700\n",
      "Epoch: 061, Train Acc: 0.8336, Test Acc: 0.8440\n",
      "Epoch: 062, Train Acc: 0.7776, Test Acc: 0.7640\n",
      "Epoch: 063, Train Acc: 0.8892, Test Acc: 0.8660\n",
      "Epoch: 064, Train Acc: 0.8824, Test Acc: 0.8620\n",
      "Epoch: 065, Train Acc: 0.9016, Test Acc: 0.8960\n",
      "Epoch: 066, Train Acc: 0.8604, Test Acc: 0.8760\n",
      "Epoch: 067, Train Acc: 0.8448, Test Acc: 0.8420\n",
      "Epoch: 068, Train Acc: 0.8468, Test Acc: 0.8520\n",
      "Epoch: 069, Train Acc: 0.8388, Test Acc: 0.8400\n",
      "Epoch: 070, Train Acc: 0.6844, Test Acc: 0.6700\n",
      "Epoch: 071, Train Acc: 0.7992, Test Acc: 0.8060\n",
      "Epoch: 072, Train Acc: 0.8924, Test Acc: 0.8940\n",
      "Epoch: 073, Train Acc: 0.8980, Test Acc: 0.8920\n",
      "Epoch: 074, Train Acc: 0.7020, Test Acc: 0.6760\n",
      "Epoch: 075, Train Acc: 0.7468, Test Acc: 0.7580\n",
      "Epoch: 076, Train Acc: 0.9032, Test Acc: 0.9080\n",
      "Epoch: 077, Train Acc: 0.6860, Test Acc: 0.6860\n",
      "Epoch: 078, Train Acc: 0.8672, Test Acc: 0.8800\n",
      "Epoch: 079, Train Acc: 0.8732, Test Acc: 0.8700\n",
      "Epoch: 080, Train Acc: 0.8032, Test Acc: 0.7700\n",
      "Epoch: 081, Train Acc: 0.8932, Test Acc: 0.8880\n",
      "Epoch: 082, Train Acc: 0.8564, Test Acc: 0.8680\n",
      "Epoch: 083, Train Acc: 0.8516, Test Acc: 0.8640\n",
      "Epoch: 084, Train Acc: 0.7896, Test Acc: 0.7980\n",
      "Epoch: 085, Train Acc: 0.8832, Test Acc: 0.8760\n",
      "Epoch: 086, Train Acc: 0.8852, Test Acc: 0.8780\n",
      "Epoch: 087, Train Acc: 0.8864, Test Acc: 0.8900\n",
      "Epoch: 088, Train Acc: 0.7372, Test Acc: 0.7620\n",
      "Epoch: 089, Train Acc: 0.7772, Test Acc: 0.7700\n",
      "Epoch: 090, Train Acc: 0.8980, Test Acc: 0.9080\n",
      "Epoch: 091, Train Acc: 0.8320, Test Acc: 0.8080\n",
      "Epoch: 092, Train Acc: 0.8828, Test Acc: 0.8840\n",
      "Epoch: 093, Train Acc: 0.5448, Test Acc: 0.5740\n",
      "Epoch: 094, Train Acc: 0.8840, Test Acc: 0.8740\n",
      "Epoch: 095, Train Acc: 0.6532, Test Acc: 0.6220\n",
      "Epoch: 096, Train Acc: 0.8500, Test Acc: 0.8560\n",
      "Epoch: 097, Train Acc: 0.8552, Test Acc: 0.8540\n",
      "Epoch: 098, Train Acc: 0.9116, Test Acc: 0.9020\n",
      "Epoch: 099, Train Acc: 0.8740, Test Acc: 0.8580\n"
     ]
    }
   ],
   "source": [
    "train_acc_lst = []\n",
    "test_acc_lst = []\n",
    "for epoch in range(1, 100):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    train_acc_lst.append(train_acc)\n",
    "    test_acc = test(test_loader)\n",
    "    test_acc_lst.append(test_acc)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd071e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x : torch.Size([5613, 32])\n",
    "# mask : torch.Size([64, 97])\n",
    "# s : torch.Size([64, 97, 20])\n",
    "# adj : torch.Size([64, 97, 97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a1804d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e2efe24",
   "metadata": {},
   "source": [
    "### Evaluate on a seperate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db5cd2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking rest graph from each population as evaluation set\n",
    "\n",
    "simu_eval_set = []\n",
    "simu_eval_labels = []\n",
    "\n",
    "for graph_set,graph_labels in zip(all_graphs_same_trial,all_graphs_labels_same_trial):\n",
    "    simu_eval_set.extend(graph_set[300:])\n",
    "    simu_eval_labels.extend(graph_labels[300:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d99adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41ab3bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7111/2408919390.py:12: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  y = torch.tensor(simu_eval_labels[i],dtype=torch.long) # add graph label\n"
     ]
    }
   ],
   "source": [
    "eval_set = []\n",
    "\n",
    "for i,g in enumerate(simu_eval_set):\n",
    "    graph_remove_dummy_nodes(g) # remove dummy nodes\n",
    "    g.remove_edges_from(nx.selfloop_edges(g)) # remove self loop edges\n",
    "    \n",
    "    attr_coords = np.array(list(nx.get_node_attributes(g,'coord').values())) #simu attribute (coords) \n",
    "    x = torch.tensor(attr_coords,dtype=torch.float)\n",
    "\n",
    "    #x = torch.tensor(nx.adjacency_matrix(g).todense(),dtype=torch.float)\n",
    "    \n",
    "    y = torch.tensor(simu_eval_labels[i],dtype=torch.long) # add graph label\n",
    "    edge_index = torch.tensor(list(g.edges))\n",
    "    \n",
    "    eval_set.append(Data(x=x, y=y, edge_index=edge_index.t().contiguous()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a08d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_set = []\n",
    "reference_label = list(np.arange(0,10,1))\n",
    "\n",
    "for i,g in enumerate(referene_graphs):\n",
    "    #graph_remove_dummy_nodes(g) \n",
    "    g.remove_edges_from(nx.selfloop_edges(g)) \n",
    "    \n",
    "    attr_coords = np.array(list(nx.get_node_attributes(g,'coord').values())) #simu attribute (coords) \n",
    "    x = torch.tensor(attr_coords,dtype=torch.float)\n",
    "    \n",
    "    y = torch.tensor(reference_label[i],dtype=torch.long) # add graph label\n",
    "    edge_index = torch.tensor(list(g.edges))\n",
    "    \n",
    "    reference_set.append(Data(x=x, y=y, edge_index=edge_index.t().contiguous()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df533628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval emb mixed with reference graph\n",
    "all_graph = DataLoader(eval_set+reference_set, batch_size= len(eval_set) + len(reference_set), shuffle=False)\n",
    "\n",
    "for data in all_graph:\n",
    "    prob,g_emb,out_emb,attn_weights = model(data.x, data.edge_index, data.batch)\n",
    "    \n",
    "# out_emb = out_emb.detach().numpy()\n",
    "# g_emb = g_emb.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed6c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_k_means(k, coords):\n",
    "    \n",
    "    kmeans = KMeans(n_init= 'auto',n_clusters=k, random_state=0).fit(coords)\n",
    "    \n",
    "    return kmeans,kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c51f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10      \n",
    "kmeans,kmeans_labels = get_labels_from_k_means(k, g_emb[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bffdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emb = np.concatenate((g_emb,kmeans.cluster_centers_),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4403fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036bef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "\n",
    "n_components = 2\n",
    "tsne = TSNE(n_components)\n",
    "\n",
    "tsne_graph = tsne.fit_transform(all_emb)\n",
    "tsne_graph.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f31cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5bab77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(13,9)})\n",
    "\n",
    "tsne_graph_df = pd.DataFrame({'tsne_1': tsne_graph[:2000,0], 'tsne_2': tsne_graph[:2000,1], 'label':simu_eval_labels})\n",
    "tsne_ref_df = pd.DataFrame({'tsne_1': tsne_graph[2000:2010,0], 'tsne_2': tsne_graph[2000:2010,1], 'label':reference_label})\n",
    "tsne_kmeans_df = pd.DataFrame({'tsne_1': tsne_graph[2010:,0], 'tsne_2': tsne_graph[2010:,1],'label':reference_label})\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "sns.scatterplot(x='tsne_1', y='tsne_2', hue='label', data=tsne_graph_df, ax=ax,s=20,palette='tab10')\n",
    "sns.scatterplot(x='tsne_1', y='tsne_2', hue='label', data=tsne_ref_df, ax=ax,s=150,palette='tab10',markers='label')\n",
    "plt.scatter(tsne_kmeans_df['tsne_1'], tsne_kmeans_df['tsne_2'], c='black', s = 100, marker='X')\n",
    "\n",
    "# lim = (tsne_graph.min()-5, tsne_graph.max()+5)\n",
    "# ax.set_xlim(lim)\n",
    "# ax.set_ylim(lim)\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1066fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a7d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ceil(0.25 * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777d2db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82971e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70e814d3",
   "metadata": {},
   "source": [
    "### Compare with classical methods on simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f77188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3670cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(g_emb, simu_eval_labels, test_size=0.6, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251474d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e3ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = SVC(gamma='auto',kernel='linear').fit(X_train, y_train)\n",
    "clf_svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier(max_depth = 4,random_state=0).fit(X_train, y_train)\n",
    "clf_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17e76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7e079",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_feature_coords = []\n",
    "\n",
    "for d in eval_set:\n",
    "    avg_feature_coords.append(np.mean(d.x.detach().numpy(),axis=0))\n",
    "    \n",
    "avg_feature_coords = np.array(avg_feature_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d633ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_coords, X_test_coords, y_train_coords, y_test_coords = train_test_split(avg_feature_coords, simu_eval_labels, test_size=0.6, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06923fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_coord = LogisticRegression(random_state=0).fit(X_train_coords, y_train_coords)\n",
    "clf_coord.score(X_test_coords, y_test_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a0834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm_coord = SVC(gamma='auto',kernel='linear').fit(X_train_coords, y_train_coords)\n",
    "clf_svm_coord.score(X_test_coords, y_test_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf_coord = RandomForestClassifier(max_depth = 5,random_state=0).fit(X_train_coords, y_train_coords)\n",
    "clf_rf_coord.score(X_test_coords, y_test_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b01e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de87403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043877d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

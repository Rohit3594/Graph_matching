# Generation of simulated graphs and result of several multiple graph matching algorithms on them

In this folder, one can find the code necessary to generate sulcal-pits like families of graph. It is also here that we provide the necessary script to compute affinity matrices and incidence matrices for all these graphs.

## Installation procedure

In order to run the above code, one need to be sure to have set the right environment. We recommend to use Anaconda to do so. The depedency are as follow (latest version runs on python 3.6):

numpy:
```sh
pip install numpy
```

networkx:
```sh
pip install networkx
```

trimesh:
```sh
pip install trimesh
```

slam:
slam is a layer that comes over trimesh and which is used to sample random points on the surface of a sphere. To use it, one first needs to clone the project from https://github.com/gauzias/slam . 
In order to make the conda environement recognize slam as an external module, one need to add a file named slam_path.pth (or any name with the extension .pth) in path/to/conda/envs/{env-name}/lib/pythonX.X/site-packages/ .  The .pth file contains one line of text that gives the absolute path to the slam module : "path/to/slam" 

## What kind of graphs are you creating ?
The idea is to create families of graphs that simulate well the structure of sulcal-pits graph. We first crate a reference graph by sampling n points from a sphere of radius 100. Then we use the convex hull represented by all these points to create the edge information of the graph. Each node gets as an attribute the corresponding coordinates on the sphere and the edge uses the geodesical distances between the two points they are attached to as attribute.

_n_ graphs are created by taking the reference graph and applying gaussian noise on his nodes attributes and reassigning the new geodesical distances for each edge. Finally outliers points are created and randomly attached to near neighbors.

## How do you calculate the affinity matrices ?

In order to calculate the affinity matrices we use a gaussian kernel on the attributes of the node/edge that we are looking for : $exp(\gamma \lVert q_1 - q_2 \rVert^2_2)$. The variable gamma is inferred for each pair by taking the median of the distances between every pair of coordinates (or geodesic distances for the edges). 
The affinity matrix is calculated for each pair of graph in a family which makes it a highly computing demanding task even if it is parallelised.

## Description of the different files

### script_generation_graphs.py

This script is used to generate a family of graphs in a specified folder with different parameters to use. To use it one needs only to give a path to the directory where all the graphs will be created. However a set of optional parameters are here to help to change important parameters that one could find of importance :
* --nb_runs: The number of families to generate for each set of parameters
* --nb_graphs : The number of graphs to generate for each family
* --nb_vertices : The original number of nodes for each graphs
* --min_noise: The minimum value for the noise parameter (which changes the deviation of the gaussian noise applied to the second graph)
* --max_noise: The maximum value for the noise parameter (in the sense of the range function, even if these values can be floats)
* --step_noise: The step size to go from min to max value of noise
* --min_outliers: The minimum number of outliers
* --max_outliers: The maximum number of outliers
* --step_outliers: The step size to go from min to max number of outliers

### script_generation_affinity_and_incidence_matrix.py
This script generates all the affinity matrices for the pairs of graph located in a given folder (generated by the generation graphs script). The only necessary argument is the path to the directory were are located the graphs. A gaussian kernel will be used and the gamma value will be inferred for each pair of graph as shortly described above. Here are the other optionnal arguments:
* --nb_workers: Decide on how many workers to launch in parallell. Each worker will generate the affinity matrices for a pair of graph.

### script_get_pairwise_good_guess.py
This script generate the matching matrices based on a low-cost "good-guess" method for all graphs in a folder. The idea is to just look at the geodesic distance between two points and assign together the two that are closest to one another. In order to avoid bias, the order in which the nodes are matched is randomised. There is one optional arguments :
* --nb_workers: Decide on how many workers to launch in parallell. Each worker will generate matching for one family of graphs.

### script_get_result_tensors.py
This script goes through the generated graphs, collects the matching of the different algorithms and calculate the accuracy compared to ground truth values. It needs two argument : the path to the graph folder and a path to write the result from the calculation (it is serialized with pickle).

### script_generate_matching_with_clustering.py
This script will use clustering to create a coherent labelling for matched graphs in a folder. This is linked to the problem of mALS being non-coherent and possible solution to solve it. The idea of this file is to use the DBSCAN algorithm on a cost matrix defined by how many GraphView match the two nodes together (or rather the inverse of this matrix). It will save the result of the algorithm as a numpy bulk matrix which match the format of the mALS one. The script will generate different clustering by using different values for the two parameters of DBSCAN (eps and min_sample). This is done to check the effect of the parameters on the final results. There are one optional parameter :
* --nb_steps: The number of different parameters values to try with DBSCAN.

### script_get_labelling_result.py
This script will look through a folder to collect the different clustering results and get the acuracy or recall of each one using ground truth values. It needs the path to the folder with clustering results and the path where to write the metric results (serialised with pickle). It also have one aditional argument:
* --use_precision: Decide if one want to use a precision metric or a recall metric (should be 0 or 1). 


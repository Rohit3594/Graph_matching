{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e07b2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f4b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import os.path as osp\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch_geometric.nn import DenseSAGEConv, dense_diff_pool,dense_mincut_pool\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/rohit/PhD_Work/GM_my_version/Graph_matching/\")\n",
    "from sklearn.cluster import KMeans\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from graph_generation.load_graphs_and_create_metadata import dataset_metadata\n",
    "from graph_matching_tools.metrics import matching\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sco\n",
    "import slam.io as sio\n",
    "from scipy.special import softmax\n",
    "import pickle\n",
    "from scipy.stats import betabinom\n",
    "import seaborn as sns\n",
    "import tools.graph_processing as gp\n",
    "import tools.graph_visu as gv\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "import torch\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from math import ceil\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import TopKPooling\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch_geometric.utils import to_dense_batch, to_dense_adj\n",
    "from torch_geometric.nn import GCNConv, DenseGraphConv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_trials = '..//data/simu_graph/simu_genders//'\n",
    "path_sub_dir = '/noise_100,outliers_varied/graphs/'\n",
    "all_trials = np.sort(list(map(int,os.listdir(path_trials)))) # all trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7134e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_graphs_same_trial = []\n",
    "all_graphs_labels_same_trial = []\n",
    "\n",
    "referene_graphs = []\n",
    "\n",
    "for trial in all_trials:\n",
    "    \n",
    "    referene_graphs.append(nx.read_gpickle(path_trials + str(trial) + '/reference_'+ str(trial) +'.gpickle'))\n",
    "    \n",
    "    trial_folder = path_trials + str(trial) + path_sub_dir\n",
    "    all_graphs_same_trial.append(gp.load_graphs_in_list(trial_folder)) # append graphs\n",
    "    all_graphs_labels_same_trial.append(np.ones([len(gp.load_graphs_in_list(trial_folder))])*trial) # append corr lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17540b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking 300 graph from each population for learning \n",
    "\n",
    "simu_train_set = []\n",
    "simu_train_labels = []\n",
    "\n",
    "for graph_set,graph_labels in zip(all_graphs_same_trial,all_graphs_labels_same_trial):\n",
    "    simu_train_set.extend(graph_set[:300])\n",
    "    simu_train_labels.extend(graph_labels[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_graphs_same_trial[0][0].nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_remove_dummy_nodes(graph):\n",
    "    nodes_dummy_true = [x for x,y in graph.nodes(data=True) if y['is_dummy']==True]\n",
    "    graph.remove_nodes_from(nodes_dummy_true)\n",
    "    #print(len(graph.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bebfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert networkx graphs to pyg graphs\n",
    "\n",
    "sulcal_simu_dataset = []\n",
    "\n",
    "for i,g in enumerate(simu_train_set):\n",
    "    #graph_remove_dummy_nodes(g) # remove dummy nodes\n",
    "    g.remove_edges_from(nx.selfloop_edges(g)) # remove self loop edges\n",
    "    \n",
    "    attr_coords = np.array(list(nx.get_node_attributes(g,'coord').values())) #simu attribute (coords)\n",
    "    \n",
    "    x = torch.tensor(attr_coords,dtype=torch.float)\n",
    "    \n",
    "    #adj = torch.tensor(nx.adjacency_matrix(g).todense(),dtype=torch.float)\n",
    "    \n",
    "    y = torch.tensor(simu_train_labels[i],dtype=torch.long) # add graph label\n",
    "    edge_index = torch.tensor(list(g.edges))\n",
    "    \n",
    "    sulcal_simu_dataset.append(Data(x=x, y=y, edge_index=edge_index.t().contiguous())) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb40a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sulcal_simu_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc3a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66127abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(sulcal_simu_dataset)\n",
    "\n",
    "train_dataset = sulcal_simu_dataset[:len(sulcal_simu_dataset)-500]\n",
    "test_dataset = sulcal_simu_dataset[len(sulcal_simu_dataset)-500:]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # dense data loader for using adjacency\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687d759",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8186f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_node_features = train_dataset[0].num_features\n",
    "num_node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac9b338",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class mincutnet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels=32):\n",
    "        super(mincutnet, self).__init__()\n",
    "        \n",
    "        self.in_head = 1\n",
    "        self.out_head = 1\n",
    "        \n",
    "        self.emb_dim = 16 # embedding dimension\n",
    "        \n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, self.emb_dim, heads=self.in_head,dropout=0.2)\n",
    "\n",
    "#        self.conv3 = GATConv(hidden_channels, self.emb_dim, concat=False, dropout=0.6)\n",
    "        \n",
    "        num_of_centers =  22\n",
    "        self.pool1 = Linear(self.emb_dim, num_of_centers) # The degree of the node belonging to any of the centers\n",
    "        \n",
    "        self.conv3 = DenseGraphConv(self.emb_dim, self.emb_dim)\n",
    "\n",
    "        #self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(self.emb_dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch): \n",
    "        \n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        #x = F.relu(self.conv2(x, edge_index))\n",
    "        node_emb, attn_weights = self.conv2(x, edge_index,return_attention_weights=True)\n",
    "        x = node_emb.relu()\n",
    "\n",
    "        x, mask = to_dense_batch(x, batch) \n",
    "        \n",
    "        adj = to_dense_adj(edge_index, batch) \n",
    "        s = self.pool1(x)\n",
    "\n",
    "        \n",
    "        x, adj, mincut_loss, ortho_loss = dense_mincut_pool(x, adj, s, mask) \n",
    "        x = self.conv3(x, adj) \n",
    "        g_emb = x.mean(dim=1) \n",
    "        #x = F.relu(self.lin1(x)) \n",
    "        out = self.lin2(g_emb)\n",
    "        return out, g_emb, node_emb, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a85c8bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = mincutnet(num_node_features, len(all_trials))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ff946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = GCN(hidden_channels=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    for data in train_loader:# Iterate in batches over the training dataset.\n",
    "        out, g_emb, node_emb, s = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        out, g_emb, node_emb, s =  model(data.x, data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        \n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b052d299",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_acc_lst = []\n",
    "test_acc_lst = []\n",
    "for epoch in range(1, 99):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    train_acc_lst.append(train_acc)\n",
    "    test_acc = test(test_loader)\n",
    "    test_acc_lst.append(test_acc)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd071e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11,6)})\n",
    "epochs = np.arange(1,99,1)\n",
    "plt.plot(epochs, train_acc_lst,label='Train accuracy')\n",
    "plt.plot(epochs, test_acc_lst, label='Test accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a1804d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e2efe24",
   "metadata": {},
   "source": [
    "### Evaluate on a seperate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5cd2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking rest graph from each population as evaluation set\n",
    "\n",
    "simu_eval_set = []\n",
    "simu_eval_labels = []\n",
    "\n",
    "for graph_set,graph_labels in zip(all_graphs_same_trial,all_graphs_labels_same_trial):\n",
    "    simu_eval_set.extend(graph_set[300:])\n",
    "    simu_eval_labels.extend(graph_labels[300:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d99adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab3bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = []\n",
    "\n",
    "for i,g in enumerate(simu_eval_set):\n",
    "    graph_remove_dummy_nodes(g) # remove dummy nodes\n",
    "    g.remove_edges_from(nx.selfloop_edges(g)) # remove self loop edges\n",
    "    \n",
    "    attr_coords = np.array(list(nx.get_node_attributes(g,'coord').values())) #simu attribute (coords) \n",
    "    x = torch.tensor(attr_coords,dtype=torch.float)\n",
    "\n",
    "    #x = torch.tensor(nx.adjacency_matrix(g).todense(),dtype=torch.float)\n",
    "    \n",
    "    y = torch.tensor(simu_eval_labels[i],dtype=torch.long) # add graph label\n",
    "    edge_index = torch.tensor(list(g.edges))\n",
    "    \n",
    "    eval_set.append(Data(x=x, y=y, edge_index=edge_index.t().contiguous()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a08d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_set = []\n",
    "reference_label = list(np.arange(0,10,1))\n",
    "\n",
    "for i,g in enumerate(referene_graphs):\n",
    "    #graph_remove_dummy_nodes(g) \n",
    "    g.remove_edges_from(nx.selfloop_edges(g)) \n",
    "    \n",
    "    attr_coords = np.array(list(nx.get_node_attributes(g,'coord').values())) #simu attribute (coords) \n",
    "    x = torch.tensor(attr_coords,dtype=torch.float)\n",
    "    \n",
    "    y = torch.tensor(reference_label[i],dtype=torch.long) # add graph label\n",
    "    edge_index = torch.tensor(list(g.edges))\n",
    "    \n",
    "    reference_set.append(Data(x=x, y=y, edge_index=edge_index.t().contiguous()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df533628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval emb mixed with reference graph\n",
    "all_graph = DataLoader(eval_set+reference_set, batch_size= len(eval_set) + len(reference_set), shuffle=False)\n",
    "\n",
    "for data in all_graph:\n",
    "      out, g_emb, node_emb, s = model(data.x, data.edge_index, data.batch)\n",
    "#   prob,g_emb,out_emb,attn_weights = model(data.x, data.edge_index, data.batch) o\n",
    "\n",
    "\n",
    "#out_emb = out_emb.detach().numpy()\n",
    "g_emb = g_emb.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b347fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed6c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_k_means(k, coords):\n",
    "    \n",
    "    kmeans = KMeans(n_init= 'auto',n_clusters=k, random_state=0).fit(coords)\n",
    "    \n",
    "    return kmeans,kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e13418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c51f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10      \n",
    "kmeans,kmeans_labels = get_labels_from_k_means(k, g_emb[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bffdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emb = np.concatenate((g_emb,kmeans.cluster_centers_),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4403fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036bef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "\n",
    "n_components = 2\n",
    "tsne = TSNE(n_components)\n",
    "\n",
    "tsne_graph = tsne.fit_transform(all_emb)\n",
    "tsne_graph.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f31cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5bab77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(13,9)})\n",
    "\n",
    "tsne_graph_df = pd.DataFrame({'tsne_1': tsne_graph[:2000,0], 'tsne_2': tsne_graph[:2000,1], 'label':simu_eval_labels})\n",
    "tsne_ref_df = pd.DataFrame({'tsne_1': tsne_graph[2000:2010,0], 'tsne_2': tsne_graph[2000:2010,1], 'label':reference_label})\n",
    "tsne_kmeans_df = pd.DataFrame({'tsne_1': tsne_graph[2010:,0], 'tsne_2': tsne_graph[2010:,1],'label':reference_label})\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "sns.scatterplot(x='tsne_1', y='tsne_2', hue='label', data=tsne_graph_df, ax=ax,s=20,palette='tab10')\n",
    "sns.scatterplot(x='tsne_1', y='tsne_2', hue='label', data=tsne_ref_df, ax=ax,s=150,palette='tab10',markers='label')\n",
    "plt.scatter(tsne_kmeans_df['tsne_1'], tsne_kmeans_df['tsne_2'], c='black', s = 100, marker='X')\n",
    "\n",
    "# lim = (tsne_graph.min()-5, tsne_graph.max()+5)\n",
    "# ax.set_xlim(lim)\n",
    "# ax.set_ylim(lim)\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1066fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a7d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ceil(0.25 * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777d2db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82971e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70e814d3",
   "metadata": {},
   "source": [
    "### Compare with classical methods on simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f77188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3670cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(g_emb[:2000], simu_eval_labels, test_size=0.6, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251474d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e3ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = SVC(gamma='auto',kernel='linear').fit(X_train, y_train)\n",
    "clf_svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier(max_depth = 5,random_state=0).fit(X_train, y_train)\n",
    "clf_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17e76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7e079",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_feature_coords = []\n",
    "\n",
    "for d in eval_set:\n",
    "    avg_feature_coords.append(np.mean(d.x.detach().numpy(),axis=0))\n",
    "    \n",
    "avg_feature_coords = np.array(avg_feature_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d633ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_coords, X_test_coords, y_train_coords, y_test_coords = train_test_split(avg_feature_coords, simu_eval_labels, test_size=0.6, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06923fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_coord = LogisticRegression(random_state=0).fit(X_train_coords, y_train_coords)\n",
    "clf_coord.score(X_test_coords, y_test_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a0834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm_coord = SVC(gamma='auto',kernel='linear').fit(X_train_coords, y_train_coords)\n",
    "clf_svm_coord.score(X_test_coords, y_test_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf_coord = RandomForestClassifier(max_depth = 5,random_state=0).fit(X_train_coords, y_train_coords)\n",
    "clf_rf_coord.score(X_test_coords, y_test_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043877d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2a6bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

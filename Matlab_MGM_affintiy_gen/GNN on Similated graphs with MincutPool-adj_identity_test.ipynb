{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e07b2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:70% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:70% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f4b194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/rohit/PhD_Work/GM_my_version/Graph_matching/\")\n",
    "import os\n",
    "from torch_geometric.datasets import TUDataset\n",
    "import os.path as osp\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch_geometric.nn import DenseSAGEConv, dense_diff_pool,dense_mincut_pool\n",
    "import torch_geometric.transforms as T\n",
    "from sklearn.cluster import KMeans\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from graph_generation.load_graphs_and_create_metadata import dataset_metadata\n",
    "from graph_matching_tools.metrics import matching\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sco\n",
    "import slam.io as sio\n",
    "from scipy.special import softmax\n",
    "import pickle\n",
    "from scipy.stats import betabinom\n",
    "import seaborn as sns\n",
    "import tools.graph_processing as gp\n",
    "import tools.graph_visu as gv\n",
    "from matplotlib.pyplot import figure\n",
    "import pandas as pd\n",
    "import random\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "import torch\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.nn.functional import one_hot\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.nn import Linear,BatchNorm1d\n",
    "import torch.nn.functional as F\n",
    "from math import ceil\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import TopKPooling\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DenseDataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch_geometric.utils import to_dense_batch, to_dense_adj\n",
    "from torch_geometric.nn import GCNConv, DenseGraphConv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fbe335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_trials = '..//data/simu_graph/Simu_high_noise_given_ref_k_1000/'\n",
    "path_sub_dir = '/noise_400,outliers_varied/graphs/'\n",
    "all_trials = np.sort(list(map(int,os.listdir(path_trials)))) # all trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a07d8de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_G(g):\n",
    "    \n",
    "#     adj_ident = nx.from_numpy_matrix(np.identity(len(g)))\n",
    "#     g_diag = nx.from_numpy_matrix(np.identity(len(g)))\n",
    "#     edge_list_iden = list(g_diag.edges)\n",
    "#     g.remove_edges_from(list(g.edges))\n",
    "#     g.add_edges_from(edge_list_iden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f7134e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_graphs_same_trial = []\n",
    "all_graphs_labels_same_trial = []\n",
    "\n",
    "referene_graphs = []\n",
    "\n",
    "for trial in all_trials:\n",
    "    \n",
    "    referene_graphs.append(nx.read_gpickle(path_trials + str(trial) + '/reference_'+ str(trial) +'.gpickle'))\n",
    "    \n",
    "    trial_folder = path_trials + str(trial) + path_sub_dir\n",
    "    all_graphs_same_trial.append(gp.load_graphs_in_list(trial_folder)) # append graphs\n",
    "    all_graphs_labels_same_trial.append(np.ones([len(gp.load_graphs_in_list(trial_folder))])*trial) # append corr lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab1cbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17540b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking 300 graph from each population for learning \n",
    "\n",
    "simu_train_set = []\n",
    "simu_train_labels = []\n",
    "\n",
    "for graph_set,graph_labels in zip(all_graphs_same_trial,all_graphs_labels_same_trial):\n",
    "    simu_train_set.extend(graph_set[:300])\n",
    "    simu_train_labels.extend(graph_labels[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c28159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0614172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_remove_dummy_nodes(graph):\n",
    "    nodes_dummy_true = [x for x,y in graph.nodes(data=True) if y['is_dummy']==True]\n",
    "    graph.remove_nodes_from(nodes_dummy_true)\n",
    "    #print(len(graph.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82bebfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_45636/3892663140.py:21: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  y = torch.tensor(simu_train_labels[i],dtype=torch.long) # add graph label\n"
     ]
    }
   ],
   "source": [
    "# Convert networkx graphs to pyg graphs\n",
    "\n",
    "sulcal_simu_dataset = []\n",
    "\n",
    "sulc_ident = []\n",
    "\n",
    "for i,g in enumerate(simu_train_set):\n",
    "    #graph_remove_dummy_nodes(g) # remove dummy nodes\n",
    "    #g.remove_edges_from(nx.selfloop_edges(g)) # remove self loop edges\n",
    "    \n",
    "    #update_G(g) # create identity adjacency matrix\n",
    "    \n",
    "    sulc_ident.append(g)\n",
    "    \n",
    "    attr_coords = np.array(list(nx.get_node_attributes(g,'coord').values())) #simu attribute (coords)\n",
    "    \n",
    "    x = torch.tensor(attr_coords,dtype=torch.float)\n",
    "    \n",
    "    #adj = torch.tensor(nx.adjacency_matrix(g).todense(),dtype=torch.float)\n",
    "    \n",
    "    y = torch.tensor(simu_train_labels[i],dtype=torch.long) # add graph label\n",
    "    edge_index = torch.tensor(list(g.edges))\n",
    "    \n",
    "    sulcal_simu_dataset.append(Data(x=x, y=y, edge_index=edge_index.t().contiguous())) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b8a342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4f8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66127abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(sulcal_simu_dataset)\n",
    "\n",
    "train_dataset = sulcal_simu_dataset[:len(sulcal_simu_dataset)-100]\n",
    "test_dataset = sulcal_simu_dataset[len(sulcal_simu_dataset)-100:]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) # dense data loader for using adjacency\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a826192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8186f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_node_features = train_dataset[0].num_features\n",
    "num_node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7750275a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create GAT Model\n",
    "\n",
    "# class MLP(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, hidden_channels=16):        \n",
    "#         super(MLP, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "        \n",
    "#         self.emb_dim = 8\n",
    "        \n",
    "#         self.lin1 = Linear(in_channels, hidden_channels)\n",
    "#         self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "#         self.lin3 = Linear(hidden_channels, out_channels)\n",
    "        \n",
    "\n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         # 1. Obtain node embeddings \n",
    "        \n",
    "#         emb = self.lin1(x)\n",
    "#         x = emb.relu()\n",
    "#         emb = self.lin2(x)\n",
    "#         x = emb.relu()\n",
    "#         g_emb = global_mean_pool(x, batch)\n",
    "#         out = self.lin3(g_emb)\n",
    "        \n",
    "#         return out, g_emb, emb, x\n",
    "\n",
    "    \n",
    "# model = MLP(num_node_features, len(all_trials))\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b66f8f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create GAT Model\n",
    "\n",
    "# class GAT(torch.nn.Module):\n",
    "#     def __init__(self, in_channels, out_channels, hidden_channels=32):        \n",
    "#         super(GAT, self).__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "#         self.in_head = 2\n",
    "#         self.out_head = 1\n",
    "        \n",
    "#         self.emb_dim = 8\n",
    "        \n",
    "        \n",
    "#         self.conv1 = GATConv(in_channels, hidden_channels, heads=self.in_head)\n",
    "#         self.conv2 = GATConv(hidden_channels*self.in_head, hidden_channels,concat=False)\n",
    "#         self.conv3 = GATConv(hidden_channels, self.emb_dim, concat=False, dropout=0.6)\n",
    "#         self.lin = Linear(self.emb_dim, out_channels)\n",
    "        \n",
    "\n",
    "#     def forward(self, x, edge_index, batch):\n",
    "#         # 1. Obtain node embeddings \n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = x.relu()\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         x = x.relu()\n",
    "#         emb,attn_weights = self.conv3(x, edge_index,return_attention_weights=True)\n",
    "\n",
    "#         # 2. Readout layer\n",
    "#         g_emb = global_mean_pool(emb, batch)  # [batch_size, hidden_channels] TopKPooling\n",
    "#         #g_emb = TopKPooling(emb, batch)\n",
    "\n",
    "#         # 3. Apply a final classifier\n",
    "#         out = F.dropout(g_emb, p=0.5, training=self.training)\n",
    "#         out = self.lin(out)\n",
    "        \n",
    "#         return out, g_emb, emb, attn_weights\n",
    "\n",
    "    \n",
    "# model = GAT(num_node_features, len(all_trials))\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7d4b94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ac9b338",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mincutnet(\n",
      "  (conv1): GCNConv(3, 32)\n",
      "  (conv2): GATConv(32, 16, heads=1)\n",
      "  (pool1): Linear(in_features=16, out_features=22, bias=True)\n",
      "  (conv3): DenseGraphConv(16, 16)\n",
      "  (lin2): Linear(in_features=16, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class mincutnet(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels=32):\n",
    "        super(mincutnet, self).__init__()\n",
    "        \n",
    "        self.in_head = 1\n",
    "        self.out_head = 1\n",
    "        \n",
    "        self.emb_dim = 16 # embedding dimension\n",
    "        \n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, self.emb_dim, heads=self.in_head,dropout=0.2)\n",
    "\n",
    "#        self.conv3 = GATConv(hidden_channels, self.emb_dim, concat=False, dropout=0.6)\n",
    "        \n",
    "        num_of_centers =  22\n",
    "        self.pool1 = Linear(self.emb_dim, num_of_centers) # The degree of the node belonging to any of the centers\n",
    "        \n",
    "        self.conv3 = DenseGraphConv(self.emb_dim, self.emb_dim)\n",
    "\n",
    "        #self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(self.emb_dim, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, batch): \n",
    "        \n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        #x = F.relu(self.conv2(x, edge_index))\n",
    "        node_emb, attn_weights = self.conv2(x, edge_index,return_attention_weights=True)\n",
    "        x = node_emb.relu()\n",
    "\n",
    "        x, mask = to_dense_batch(x, batch) \n",
    "        \n",
    "        adj = to_dense_adj(edge_index, batch) \n",
    "        s = self.pool1(x)\n",
    "\n",
    "        \n",
    "        x, adj, mincut_loss, ortho_loss = dense_mincut_pool(x, adj, s, mask) \n",
    "        x = self.conv3(x, adj) \n",
    "        g_emb = x.mean(dim=1) \n",
    "        #x = F.relu(self.lin1(x)) \n",
    "        out = self.lin2(g_emb)\n",
    "        return out, g_emb, node_emb, attn_weights\n",
    "\n",
    "model = mincutnet(num_node_features, len(all_trials))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a85c8bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b0ff946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = GCN(hidden_channels=32)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    for data in train_loader:# Iterate in batches over the training dataset.\n",
    "        out, g_emb, node_emb, s = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        out, g_emb, node_emb, s =  model(data.x, data.edge_index, data.batch)  \n",
    "        pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        \n",
    "    return correct / len(loader.dataset)  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b052d299",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train Acc: 0.5060, Test Acc: 0.4700\n",
      "Epoch: 002, Train Acc: 0.4940, Test Acc: 0.5300\n",
      "Epoch: 003, Train Acc: 0.5060, Test Acc: 0.4700\n",
      "Epoch: 004, Train Acc: 0.5020, Test Acc: 0.5100\n",
      "Epoch: 005, Train Acc: 0.4940, Test Acc: 0.5300\n",
      "Epoch: 006, Train Acc: 0.5100, Test Acc: 0.5000\n",
      "Epoch: 007, Train Acc: 0.5080, Test Acc: 0.4600\n",
      "Epoch: 008, Train Acc: 0.5060, Test Acc: 0.4700\n",
      "Epoch: 009, Train Acc: 0.5200, Test Acc: 0.5200\n",
      "Epoch: 010, Train Acc: 0.5660, Test Acc: 0.4800\n",
      "Epoch: 011, Train Acc: 0.5620, Test Acc: 0.4700\n",
      "Epoch: 012, Train Acc: 0.5220, Test Acc: 0.5400\n",
      "Epoch: 013, Train Acc: 0.5080, Test Acc: 0.4900\n",
      "Epoch: 014, Train Acc: 0.5420, Test Acc: 0.5200\n",
      "Epoch: 015, Train Acc: 0.5540, Test Acc: 0.5400\n",
      "Epoch: 016, Train Acc: 0.5600, Test Acc: 0.5100\n",
      "Epoch: 017, Train Acc: 0.5040, Test Acc: 0.5000\n",
      "Epoch: 018, Train Acc: 0.5680, Test Acc: 0.5300\n",
      "Epoch: 019, Train Acc: 0.5720, Test Acc: 0.5400\n",
      "Epoch: 020, Train Acc: 0.5320, Test Acc: 0.5000\n",
      "Epoch: 021, Train Acc: 0.5680, Test Acc: 0.5100\n",
      "Epoch: 022, Train Acc: 0.5480, Test Acc: 0.5400\n",
      "Epoch: 023, Train Acc: 0.5740, Test Acc: 0.5200\n",
      "Epoch: 024, Train Acc: 0.5560, Test Acc: 0.5300\n",
      "Epoch: 025, Train Acc: 0.5480, Test Acc: 0.5400\n",
      "Epoch: 026, Train Acc: 0.5700, Test Acc: 0.5300\n",
      "Epoch: 027, Train Acc: 0.5180, Test Acc: 0.4900\n",
      "Epoch: 028, Train Acc: 0.6000, Test Acc: 0.5400\n",
      "Epoch: 029, Train Acc: 0.5540, Test Acc: 0.5400\n",
      "Epoch: 030, Train Acc: 0.5620, Test Acc: 0.5500\n",
      "Epoch: 031, Train Acc: 0.5660, Test Acc: 0.5900\n",
      "Epoch: 032, Train Acc: 0.5320, Test Acc: 0.4900\n",
      "Epoch: 033, Train Acc: 0.5800, Test Acc: 0.5800\n",
      "Epoch: 034, Train Acc: 0.5860, Test Acc: 0.5700\n",
      "Epoch: 035, Train Acc: 0.5900, Test Acc: 0.5600\n",
      "Epoch: 036, Train Acc: 0.5920, Test Acc: 0.5900\n",
      "Epoch: 037, Train Acc: 0.6020, Test Acc: 0.5800\n",
      "Epoch: 038, Train Acc: 0.5880, Test Acc: 0.5400\n",
      "Epoch: 039, Train Acc: 0.5980, Test Acc: 0.6000\n",
      "Epoch: 040, Train Acc: 0.5900, Test Acc: 0.5600\n",
      "Epoch: 041, Train Acc: 0.5880, Test Acc: 0.5600\n",
      "Epoch: 042, Train Acc: 0.5960, Test Acc: 0.5800\n",
      "Epoch: 043, Train Acc: 0.6220, Test Acc: 0.6500\n",
      "Epoch: 044, Train Acc: 0.6020, Test Acc: 0.5200\n",
      "Epoch: 045, Train Acc: 0.6040, Test Acc: 0.5800\n",
      "Epoch: 046, Train Acc: 0.5580, Test Acc: 0.5000\n",
      "Epoch: 047, Train Acc: 0.6340, Test Acc: 0.6300\n",
      "Epoch: 048, Train Acc: 0.6020, Test Acc: 0.5200\n",
      "Epoch: 049, Train Acc: 0.5840, Test Acc: 0.5700\n",
      "Epoch: 050, Train Acc: 0.5860, Test Acc: 0.5000\n",
      "Epoch: 051, Train Acc: 0.6280, Test Acc: 0.5800\n",
      "Epoch: 052, Train Acc: 0.6040, Test Acc: 0.5100\n",
      "Epoch: 053, Train Acc: 0.6100, Test Acc: 0.5700\n",
      "Epoch: 054, Train Acc: 0.6220, Test Acc: 0.5300\n",
      "Epoch: 055, Train Acc: 0.6300, Test Acc: 0.5000\n",
      "Epoch: 056, Train Acc: 0.6180, Test Acc: 0.5900\n",
      "Epoch: 057, Train Acc: 0.5460, Test Acc: 0.4800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m test_acc_lst \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m99\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     train_acc \u001b[38;5;241m=\u001b[39m test(train_loader)\n\u001b[1;32m      6\u001b[0m     train_acc_lst\u001b[38;5;241m.\u001b[39mappend(train_acc)\n",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m():\n\u001b[1;32m      6\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\u001b[38;5;66;03m# Iterate in batches over the training dataset.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         out, g_emb, node_emb, s \u001b[38;5;241m=\u001b[39m model(data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mbatch)  \u001b[38;5;66;03m# Perform a single forward pass.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(out, data\u001b[38;5;241m.\u001b[39my)  \u001b[38;5;66;03m# Compute the loss.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Slam_python_3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/Slam_python_3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/Slam_python_3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Slam_python_3.8/lib/python3.8/site-packages/torch_geometric/loader/dataloader.py:19\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     17\u001b[0m elem \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, BaseData):\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[0;32m~/anaconda3/envs/Slam_python_3.8/lib/python3.8/site-packages/torch_geometric/data/batch.py:76\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[0;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_data_list\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data_list: List[BaseData],\n\u001b[1;32m     66\u001b[0m                    follow_batch: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     67\u001b[0m                    exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    Python list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m    :obj:`follow_batch`.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincrement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)\n\u001b[1;32m     86\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict\n",
      "File \u001b[0;32m~/anaconda3/envs/Slam_python_3.8/lib/python3.8/site-packages/torch_geometric/data/collate.py:84\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Collate attributes into a unified representation:\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m value, slices, incs \u001b[38;5;241m=\u001b[39m \u001b[43m_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mincrement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, Tensor) \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mis_cuda:\n\u001b[1;32m     88\u001b[0m     device \u001b[38;5;241m=\u001b[39m value\u001b[38;5;241m.\u001b[39mdevice\n",
      "File \u001b[0;32m~/anaconda3/envs/Slam_python_3.8/lib/python3.8/site-packages/torch_geometric/data/collate.py:155\u001b[0m, in \u001b[0;36m_collate\u001b[0;34m(key, values, data_list, stores, increment)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcat_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value, slices, incs\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, SparseTensor) \u001b[38;5;129;01mand\u001b[39;00m increment:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Concatenate a list of `SparseTensor` along the `cat_dim`.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# NOTE: `cat_dim` may return a tuple to allow for diagonal stacking.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_acc_lst = []\n",
    "test_acc_lst = []\n",
    "for epoch in range(1, 99):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    train_acc_lst.append(train_acc)\n",
    "    test_acc = test(test_loader)\n",
    "    test_acc_lst.append(test_acc)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cd071e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sns.set(rc={'figure.figsize':(11,6)})\n",
    "# epochs = np.arange(1,99,1)\n",
    "# plt.plot(epochs, train_acc_lst,label='Train accuracy')\n",
    "# plt.plot(epochs, test_acc_lst, label='Test accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend(loc=\"lower right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a1804d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e2efe24",
   "metadata": {},
   "source": [
    "### Evaluate on a seperate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5cd2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking rest graph from each population as evaluation set\n",
    "\n",
    "simu_eval_set = []\n",
    "simu_eval_labels = []\n",
    "\n",
    "for graph_set,graph_labels in zip(all_graphs_same_trial,all_graphs_labels_same_trial):\n",
    "    simu_eval_set.extend(graph_set[300:])\n",
    "    simu_eval_labels.extend(graph_labels[300:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d99adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab3bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = []\n",
    "\n",
    "for i,g in enumerate(simu_eval_set):\n",
    "    graph_remove_dummy_nodes(g) # remove dummy nodes\n",
    "#     g.remove_edges_from(nx.selfloop_edges(g)) # remove self loop edges\n",
    "    #update_G(g)\n",
    "    \n",
    "    attr_coords = np.array(list(nx.get_node_attributes(g,'coord').values())) #simu attribute (coords) \n",
    "    x = torch.tensor(attr_coords,dtype=torch.float)\n",
    "\n",
    "    #x = torch.tensor(nx.adjacency_matrix(g).todense(),dtype=torch.float)\n",
    "    \n",
    "    y = torch.tensor(simu_eval_labels[i],dtype=torch.long) # add graph label\n",
    "    edge_index = torch.tensor(list(g.edges))\n",
    "    \n",
    "    eval_set.append(Data(x=x, y=y, edge_index=edge_index.t().contiguous()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e52623e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a08d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference_set = []\n",
    "# reference_label = list(np.arange(0,10,1))\n",
    "\n",
    "# for i,g in enumerate(referene_graphs):\n",
    "#     #graph_remove_dummy_nodes(g) \n",
    "#     #g.remove_edges_from(nx.selfloop_edges(g)) \n",
    "    \n",
    "#     update_G(g)\n",
    "    \n",
    "#     attr_coords = np.array(list(nx.get_node_attributes(g,'coord').values())) #simu attribute (coords) \n",
    "#     x = torch.tensor(attr_coords,dtype=torch.float)\n",
    "    \n",
    "#     y = torch.tensor(reference_label[i],dtype=torch.long) # add graph label\n",
    "#     edge_index = torch.tensor(list(g.edges))\n",
    "    \n",
    "#     reference_set.append(Data(x=x, y=y, edge_index=edge_index.t().contiguous()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df533628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval emb mixed with reference graph\n",
    "#all_graph = DataLoader(eval_set+reference_set, batch_size= len(eval_set) + len(reference_set), shuffle=False)\n",
    "all_graph = DataLoader(eval_set, batch_size= len(eval_set), shuffle=False)\n",
    "\n",
    "for data in all_graph:\n",
    "      out, g_emb, node_emb, s = model(data.x, data.edge_index, data.batch)\n",
    "#   prob,g_emb,out_emb,attn_weights = model(data.x, data.edge_index, data.batch) o\n",
    "\n",
    "\n",
    "#out_emb = out_emb.detach().numpy()\n",
    "g_emb = g_emb.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b347fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ed6c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_k_means(k, coords):\n",
    "    \n",
    "    kmeans = KMeans(n_init= 'auto',n_clusters=k, random_state=0).fit(coords)\n",
    "    \n",
    "    return kmeans,kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e13418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89c51f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "kmeans,kmeans_labels = get_labels_from_k_means(k, g_emb[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bffdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_emb = np.concatenate((g_emb,kmeans.cluster_centers_),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036bef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "\n",
    "n_components = 2\n",
    "tsne = TSNE(n_components)\n",
    "\n",
    "tsne_graph = tsne.fit_transform(all_emb)\n",
    "tsne_graph.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f31cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5bab77",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(13,9)})\n",
    "\n",
    "tsne_graph_df = pd.DataFrame({'tsne_1': tsne_graph[:400,0], 'tsne_2': tsne_graph[:400,1], 'label':simu_eval_labels})\n",
    "#tsne_ref_df = pd.DataFrame({'tsne_1': tsne_graph[2000:2010,0], 'tsne_2': tsne_graph[2000:2010,1], 'label':reference_label})\n",
    "tsne_kmeans_df = pd.DataFrame({'tsne_1': tsne_graph[400:,0], 'tsne_2': tsne_graph[400:,1],'label':[0,1]})\n",
    "\n",
    "fig, ax = plt.subplots(1)\n",
    "sns.scatterplot(x='tsne_1', y='tsne_2', hue='label', data=tsne_graph_df, ax=ax,s=90,palette='tab10')\n",
    "#sns.scatterplot(x='tsne_1', y='tsne_2', hue='label', data=tsne_ref_df, ax=ax,s=150,palette='tab10',markers='label')\n",
    "plt.scatter(tsne_kmeans_df['tsne_1'], tsne_kmeans_df['tsne_2'], c='black', s = 100, marker='X')\n",
    "\n",
    "# lim = (tsne_graph.min()-5, tsne_graph.max()+5)\n",
    "# ax.set_xlim(lim)\n",
    "# ax.set_ylim(lim)\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a777d2db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82971e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70e814d3",
   "metadata": {},
   "source": [
    "### Compare with classical methods on simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f77188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3670cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(g_emb[:2000], simu_eval_labels, test_size=0.6, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251474d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e3ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = SVC(gamma='auto',kernel='linear').fit(X_train, y_train)\n",
    "clf_svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = RandomForestClassifier(max_depth = 5,random_state=0).fit(X_train, y_train)\n",
    "clf_rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17e76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7e079",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_feature_coords = []\n",
    "\n",
    "for d in eval_set:\n",
    "    avg_feature_coords.append(np.mean(d.x.detach().numpy(),axis=0))\n",
    "    \n",
    "avg_feature_coords = np.array(avg_feature_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d633ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_coords, X_test_coords, y_train_coords, y_test_coords = train_test_split(avg_feature_coords, simu_eval_labels, test_size=0.6, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06923fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_coord = LogisticRegression(random_state=0).fit(X_train_coords, y_train_coords)\n",
    "clf_coord.score(X_test_coords, y_test_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a0834e",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm_coord = SVC(gamma='auto',kernel='linear').fit(X_train_coords, y_train_coords)\n",
    "clf_svm_coord.score(X_test_coords, y_test_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4a0767",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf_coord = RandomForestClassifier(max_depth = 5,random_state=0).fit(X_train_coords, y_train_coords)\n",
    "clf_rf_coord.score(X_test_coords, y_test_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91b01e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de87403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043877d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

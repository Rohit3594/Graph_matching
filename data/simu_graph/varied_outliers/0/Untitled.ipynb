{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import betabinom\n",
    "\n",
    "\n",
    "def compute_beta(alpha, n, mean):\n",
    "    return (1-mean/n) / (mean/n) * alpha\n",
    "\n",
    "\n",
    "def compute_alpha(n, mean, variance):\n",
    "    ratio = (1-mean/n) / (mean/n)\n",
    "    alpha = ((1+ratio)**2 * variance - n**2 * ratio) / (n*ratio*(1+ratio) - variance* (1 + ratio)**3)\n",
    "    return alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(19680801)\n",
    "\n",
    "    # example data\n",
    "    n = 30  # Size of the support (i.e. numbers between 0 and n)\n",
    "    mu = 10  # mean of distribution\n",
    "    sigma = 4  # standard deviation of distribution\n",
    "    \n",
    "    # Compute the alpha and beta for the given mu and sigma\n",
    "    alpha = compute_alpha(n , mu, sigma**2)\n",
    "    beta = compute_beta(alpha, n, mu)\n",
    "    \n",
    "    x = betabinom.rvs(n, alpha, beta, size=100000)\n",
    "\n",
    "    num_bins = n+1\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # the histogram of the data\n",
    "    n, bins, patches = ax.hist(x, num_bins, density=True)\n",
    "\n",
    "    ax.set_xlabel('Number of outliers')\n",
    "    ax.set_ylabel('Probability density')\n",
    "    ax.set_title(r'Histogram of Beta-binominal dist: $\\mu=8$, $\\sigma=4$')\n",
    "\n",
    "    # Tweak spacing to prevent clipping of ylabel\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/rohit/PhD_Work/GM_my_version/Graph_matching/\")\n",
    "import argparse\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import slam.plot as splt\n",
    "import slam.topology as stop\n",
    "import slam.generate_parametric_surfaces as sgps\n",
    "import trimesh\n",
    "import os\n",
    "import tools.graph_processing as gp\n",
    "from sphere import *\n",
    "from tqdm.auto import tqdm,trange\n",
    "from scipy.stats import betabinom\n",
    "import random\n",
    "import pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sphere_random_sampling(vertex_number=100, radius=1.0):\n",
    "    \"\"\"\n",
    "\tgenerate a sphere with random sampling\n",
    "\t:param vertex_number: number of vertices in the output spherical mesh\n",
    "\t:param radius: radius of the output sphere\n",
    "\t:return:\n",
    "\t\"\"\"\n",
    "    coords = np.zeros((vertex_number, 3))\n",
    "    for i in range(vertex_number):\n",
    "        M = np.random.normal(size=(3, 3))\n",
    "        Q, R = np.linalg.qr(M)\n",
    "        coords[i, :] = Q[:, 0].transpose() * np.sign(R[0, 0])\n",
    "    if radius != 1:\n",
    "        coords = radius * coords\n",
    "    return coords\n",
    "\n",
    "def tri_from_hull(vertices):\n",
    "    \"\"\"\n",
    "\tcompute faces from vertices using trimesh convex hull\n",
    "\t:param vertices: (n, 3) float\n",
    "\t:return:\n",
    "\t\"\"\"\n",
    "    mesh = trimesh.Trimesh(vertices=vertices, process=False)\n",
    "    return mesh.convex_hull\n",
    "\n",
    "\n",
    "def edge_len_threshold(graph,thr): # Adds a percentage of edges \n",
    "    \n",
    "    edge_to_add = random.sample(list(graph.edges),round(len(graph.edges)*thr))\n",
    "\n",
    "    return edge_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_beta(alpha, n, mean):\n",
    "    return (1-mean/n) / (mean/n) * alpha\n",
    "\n",
    "\n",
    "def compute_alpha(n, mean, variance):\n",
    "    ratio = (1-mean/n) / (mean/n)\n",
    "    alpha = ((1+ratio)**2 * variance - n**2 * ratio) / (n*ratio*(1+ratio) - variance* (1 + ratio)**3)\n",
    "    return alpha\n",
    "\n",
    "\n",
    "def generate_nb_outliers_and_nb_supress(nb_vertices):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Sample nb_outliers and nb_supress from a Normal dist\n",
    "    following the std of real data\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    mean_real_data = 10         # mean real data\n",
    "    std_real_data = 4           # std real data\n",
    "\n",
    "\n",
    "    mu = 10 # mu_A = mu_B = mu\n",
    "    sigma = std_real_data\n",
    "    n = 20\n",
    "\n",
    "    alpha = compute_alpha(n , mu, sigma**2)  # corresponding alpha with respect to given mu and sigma\n",
    "    beta = compute_beta(alpha, n, mu)       # corresponding beta\n",
    "\n",
    "    nb_supress = betabinom.rvs(n, alpha, beta, size=1)[0]\n",
    "    nb_outliers = betabinom.rvs(n, alpha, beta, size=1)[0]                 # Sample nb_outliers\n",
    "\n",
    "\n",
    "    return int(nb_outliers),int(nb_supress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_graph(original_graph, nb_vertices, sigma_noise_nodes=1400, sigma_noise_edges=1,\n",
    "                         radius=100):\n",
    "    # Perturbate the coordinates\n",
    "\n",
    "    noisy_coord = []\n",
    "    key = []\n",
    "    value = []\n",
    "    nb_outliers = 0\n",
    "    nb_supress = 0\n",
    "\n",
    "    for index in range(nb_vertices):\n",
    "        # Sampling from Von Mises - Fisher distribution\n",
    "        original_coord = original_graph.nodes[index][\"coord\"]\n",
    "        mean_original = original_coord / np.linalg.norm(original_coord)  # convert to mean unit vector.\n",
    "        noisy_coordinate = Sphere().sample(1, distribution='vMF', mu=mean_original,\n",
    "                                           kappa=sigma_noise_nodes).sample[0]\n",
    "\n",
    "        noisy_coordinate = noisy_coordinate * np.linalg.norm(original_coord) # rescale to original size.\n",
    "        # print(noisy_coordinate)\n",
    "        noisy_coord.append(noisy_coordinate)\n",
    "\n",
    "        print(original_coord,\" \",noisy_coordinate)\n",
    "\n",
    "\n",
    "    nb_outliers, nb_supress = generate_nb_outliers_and_nb_supress(nb_vertices)  # Sample nb_outliers and nb_supress\n",
    "    nb_outliers = 0\n",
    "    \n",
    "    noisy_coord_all = noisy_coord\n",
    "    \n",
    "\n",
    "    #Supress Non-Outlier nodes\n",
    "    if nb_supress > 0:\n",
    "        \n",
    "        supress_list = random.sample(range(len(noisy_coord)), nb_supress) # Indexes to remove \n",
    "        \n",
    "        removed_coords = [noisy_coord[i] for i in range(len(noisy_coord)) if i in supress_list]\n",
    "        \n",
    "        #noisy_coord = [dummy_coords if i in supress_list else noisy_coord[i] for i in range(len(noisy_coord))]\n",
    "        \n",
    "        noisy_coord = [noisy_coord[i] for i in range(len(noisy_coord)) if i not in supress_list]\n",
    "        \n",
    "        print(\"nb_supress : \",nb_supress)\n",
    "        \n",
    "            \n",
    "    #print('Noisy coord len after supression',len(noisy_coord_all))\n",
    "    #print('removed coord len after supression',len(removed_coords))\n",
    "\n",
    "\n",
    "    # Add Outliers\n",
    "    sphere_random_sampling = []\n",
    "    if nb_outliers > 0:\n",
    "        #print(\"nb_outliers: \", nb_outliers)\n",
    "\n",
    "        sphere_random_sampling = generate_sphere_random_sampling(vertex_number=nb_outliers, radius=radius)\n",
    "        # merge pertubated and outlier coordinates to add edges \n",
    "        all_coord = noisy_coord + list(sphere_random_sampling)\n",
    "    else:\n",
    "        all_coord = noisy_coord\n",
    "\n",
    "\n",
    "    noisy_graph = nx.Graph()\n",
    "\n",
    "    compute_noisy_edges = tri_from_hull(all_coord)  # take all peturbated coord and comp conv hull.\n",
    "    adja = stop.adjacency_matrix(compute_noisy_edges)  # compute the new adjacency mat.\n",
    "\n",
    "    noisy_graph = nx.from_numpy_matrix(adja.todense())\n",
    "    \n",
    "\n",
    "    node_attribute_dict = {}\n",
    "    for node in noisy_graph.nodes():\n",
    "        node_attribute_dict[node] = {\"coord\": np.array(compute_noisy_edges.vertices[node]),'is_dummy':False}\n",
    "        \n",
    "#     dummy_nodes = [j for j in range(len(noisy_graph.nodes), len(noisy_graph.nodes) + len(supress_list))] \n",
    "#     noisy_graph.add_nodes_from(dummy_nodes,is_dummy=True) # add dummy nodes\n",
    "    \n",
    "        \n",
    "    nx.set_node_attributes(noisy_graph, node_attribute_dict)\n",
    "    nx.set_edge_attributes(noisy_graph, 1.0, name=\"weight\")\n",
    "\n",
    "    \n",
    "    edge_attribute_dict = {}\n",
    "    id_counter = 0  # useful for affinity matrix caculation\n",
    "    for edge in noisy_graph.edges:\n",
    "        # We calculate the geodesic distance\n",
    "        end_a = noisy_graph.nodes()[edge[0]][\"coord\"]\n",
    "        end_b = noisy_graph.nodes()[edge[1]][\"coord\"]\n",
    "        geodesic_dist = gp.compute_geodesic_distance_sphere(end_a, end_b, radius)\n",
    "\n",
    "        # add the information in the dictionnary\n",
    "        edge_attribute_dict[edge] = {\"geodesic_distance\": geodesic_dist, \"id\": id_counter}\n",
    "        id_counter += 1\n",
    "\n",
    "    # add the edge attributes to the graph\n",
    "    nx.set_edge_attributes(noisy_graph, edge_attribute_dict)\n",
    "\n",
    "    # Extracting the ground-truth correspondence\n",
    "\n",
    "    \n",
    "    ground_truth_permutation = []\n",
    "    counter = 0\n",
    "    check = False\n",
    "#     print(dummy_nodes)\n",
    "    \n",
    "    for i in range(len(noisy_graph.nodes)): \n",
    "        for j in range(len(noisy_coord_all)):  # upto the indexes of outliers\n",
    "            \n",
    "            if np.linalg.norm(noisy_coord_all[j] - noisy_graph.nodes[i]['coord']) == 0.:\n",
    "                #check = True\n",
    "                \n",
    "                if j >= len(noisy_coord_all):\n",
    "                    key.append(i)\n",
    "                ground_truth_permutation.append(j)\n",
    "        \n",
    "#         if check == False:\n",
    "#             print(counter)\n",
    "            \n",
    "#             if dummy_nodes[counter] >= len(noisy_coord_all):\n",
    "#                 key.append(i)\n",
    "#             ground_truth_permutation.append(dummy_nodes[counter])#add idx of dummy node as correspondence\n",
    "#             counter += 1\n",
    "#         check = False\n",
    "                \n",
    "            \n",
    "\n",
    "    for outlier in sphere_random_sampling:\n",
    "        for i in range(len(all_coord)):\n",
    "            if np.mean(noisy_graph.nodes[i]['coord']) == np.mean(outlier):\n",
    "                if i<nb_vertices:\n",
    "                    value.append(i)\n",
    "\n",
    "\n",
    "    if nb_outliers > 0 and len(key)!=0:\n",
    "        index = 0\n",
    "        for j in range(len(ground_truth_permutation)):\n",
    "            if ground_truth_permutation[j] == key[index]:\n",
    "                ground_truth_permutation[j] = value[index]\n",
    "                index+=1\n",
    "                if index == len(key):\n",
    "                    break\n",
    "\n",
    "        key = key + value\n",
    "        value = value + key\n",
    "\n",
    "        mapping = dict(zip(key,value))\n",
    "        #print(\"mapping :\",mapping)\n",
    "        #print(\"number of nodes in graphs: \", len(noisy_graph.nodes))\n",
    "        noisy_graph = nx.relabel_nodes(noisy_graph, mapping)\n",
    "\n",
    "\n",
    "    # Remove 10% of random edges\n",
    "#     edge_to_remove = edge_len_threshold(noisy_graph, 0.10)\n",
    "#     noisy_graph.remove_edges_from(edge_to_remove)\n",
    "\n",
    "#     noisy_graph.remove_edges_from(nx.selfloop_edges(noisy_graph))\n",
    "\n",
    "\n",
    "    return ground_truth_permutation, noisy_graph,noisy_coord_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_graphs= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_graph = nx.read_gpickle('reference_1.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 20.96346116 -76.88340545 -60.41088695]   [ 19.09488369 -78.80050761 -58.53089285]\n",
      "[-40.38208717  36.61107826  83.83863062]   [-39.12351121  39.7561781   82.99877815]\n",
      "[-53.14616772  26.3147491  -80.51719591]   [-52.93010636  31.44285046 -78.8019733 ]\n",
      "[87.30901573 46.98534633 13.01971589]   [87.9771538  45.44619989 13.95217996]\n",
      "[-50.93066397  85.66396199  -8.23122617]   [-53.43257441  84.13497556  -8.14038578]\n",
      "[-85.9712732   50.983334    -3.10480883]   [-85.24276973  52.18966108  -3.14793333]\n",
      "[-83.8724879  -23.10325171 -49.31171803]   [-81.18550885 -26.20220337 -52.17621768]\n",
      "[ -8.14699669  97.86962485 -18.84576812]   [ -7.77793334  98.16851318 -17.39099692]\n",
      "[-90.38310799  25.9637659   34.01141941]   [-91.0023121   24.8848667   33.15603417]\n",
      "[ 2.31108465 83.28236347 55.30557678]   [ 2.70504306 83.40216573 55.10681894]\n",
      "[84.6487079  14.32457521 51.27770271]   [84.4786324  14.06111058 51.62989286]\n",
      "[ 81.88240267 -57.22654565   4.51603881]   [ 80.04609552 -59.85900791   3.0857356 ]\n",
      "[ 20.71161998  32.4404267  -92.29651951]   [ 23.07715084  33.52948453 -91.34122167]\n",
      "[ 71.1577078   65.44426721 -25.56615945]   [ 71.07525421  65.9567514  -24.45434897]\n",
      "[-39.92334919 -43.92600033 -80.47752906]   [-42.32844044 -42.1878052  -80.17787863]\n",
      "[ 67.64475611 -64.17671898 -36.13219771]   [ 68.93882488 -62.46248256 -36.68619217]\n",
      "[ 37.35079912 -92.41109706  -8.06888744]   [ 36.91877807 -92.40256309  -9.93831773]\n",
      "[ 53.60484077 -23.35121063 -81.12485444]   [ 55.67044812 -25.42426773 -79.08481407]\n",
      "[-56.41173779  75.65470627  33.07689918]   [-54.78638933  76.02517174  34.90880701]\n",
      "[ 44.5827481  -72.2409887   52.85468875]   [ 45.33317221 -70.13068594  55.01445616]\n",
      "nb_supress :  16\n"
     ]
    }
   ],
   "source": [
    "ground_truth_3, noisy_graph_3,noisy_coord_all_3 = generate_noisy_graph(ref_graph,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_in_between_perm_matrix(perm_mat_1, perm_mat_2):\n",
    "    \"\"\"\n",
    "\tGiven two permutation from noisy graphs to a reference graph,\n",
    "\tReturn the permutation matrix to go from one graph to the other\n",
    "\t\"\"\"\n",
    "    result_perm = {}\n",
    "    for i in range(len(perm_mat_1)):\n",
    "        for j in range(len(perm_mat_2)):\n",
    "            if perm_mat_1[i] == perm_mat_2[j]:\n",
    "                result_perm[i] = j\n",
    "\n",
    "    return result_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_0 = ground_truth\n",
    "gt_1 = ground_truth_1\n",
    "gt_2 = ground_truth_2\n",
    "gt_3 = ground_truth_3\n",
    "gt_all = [gt_0,gt_1,gt_2,gt_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_groundtruth = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_all = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_03 = {}\n",
    "\n",
    "for i in range(len(gt_0)):\n",
    "    for j in range(len(gt_3)):\n",
    "        if gt_0[i] == gt_3[j]:\n",
    "            gt_03[i] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_groundtruth = []\n",
    "new_groundtruth = [gt_00,gt_01,gt_02,gt_03]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_dict = {}\n",
    "\n",
    "for i in range(1):\n",
    "    for j in range(4):\n",
    "        ground_dict[str(i)+str(j)] =globals()['gt_'+str(i)+str(j)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'00': {0: 0,\n",
       "  1: 1,\n",
       "  2: 2,\n",
       "  3: 3,\n",
       "  4: 4,\n",
       "  5: 5,\n",
       "  6: 6,\n",
       "  7: 7,\n",
       "  8: 8,\n",
       "  9: 9,\n",
       "  10: 10,\n",
       "  11: 11,\n",
       "  12: 12,\n",
       "  13: 13,\n",
       "  14: 14,\n",
       "  15: 15,\n",
       "  16: 16,\n",
       "  17: 17},\n",
       " '01': {1: 5, 3: 1, 6: 2, 7: 0, 9: 3},\n",
       " '02': {0: 6, 1: 2, 4: 9, 5: 1, 8: 4, 10: 7, 11: 5, 13: 0, 16: 3},\n",
       " '03': {9: 2, 10: 3, 13: 1, 16: 0}}"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-85.58139647 -22.13029838 -46.75547531] [-82.71397989 -25.92266759 -49.86394324]\n",
      "[-40.18879963 -42.20336325 -81.26337745] [-41.50888635 -43.54477485 -79.88031633]\n",
      "[-51.60210873  27.43624786 -81.14477604] [-51.86717755  28.98825999 -80.43305711]\n",
      "[ 23.93887065 -76.9840503  -59.16406402] [ 23.27153691 -76.75259169 -59.72834535]\n",
      "[82.84766059 11.25818267 54.85907817] [83.71842446 13.34072409 53.04008377]\n",
      "[ 53.01798254 -24.54752082 -81.1573333 ] [ 53.44517289 -24.16953847 -80.99041243]\n",
      "[ -6.63701392  98.15520064 -17.93060607] [-13.99191663  97.54494265 -17.00618806]\n",
      "[ 70.18409684  66.53387782 -25.4447569 ] [ 70.12577803  65.54382085 -28.04251779]\n",
      "[ 23.32676085  33.96232178 -91.11763236] [ 14.92019056  30.41685869 -94.08614468]\n"
     ]
    }
   ],
   "source": [
    "for key,value in gt_02.items():\n",
    "    print(noisy_graph.nodes[key]['coord'],noisy_graph_2.nodes[value]['coord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key,value in g3_dict.items():\n",
    "#     if value == -1:\n",
    "#         continue\n",
    "#     else:\n",
    "#         print(noisy_graph_3.nodes[key]['coord'],'-----',value,'----',noisy_graph.nodes[value]['coord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ref_graph.nodes)):\n",
    "    print(ref_graph.nodes[i]['coord'],noisy_coord_all[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ref_graph.nodes)):\n",
    "    print(ref_graph.nodes[i]['coord'],noisy_coord_all_3[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.read_gpickle('test_graphs.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.load('new_groundtruth.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graphs = [noisy_graph,noisy_graph_1,noisy_graph_2,noisy_graph_3]\n",
    "np.save('new_groundtruth.npy',new_groundtruth)\n",
    "nx.write_gpickle(test_graphs,'test_graphs.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_graph_2.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('g3 node attr len: ',nx.get_node_attributes(g1,'label_gt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in enumerate(ground_truth_permutation):\n",
    "    print(noisy_graph.nodes[i]['coord'],noisy_coord_all[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shuffled_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_coord_all[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_graph.nodes[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in enumerate(ground_truth):\n",
    "    \n",
    "    if noisy_graph.nodes[j]['is_dummy'] == False:\n",
    "        \n",
    "        print(noisy_graph.nodes[j]['coord'],ref_graph.nodes[i]['coord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = random.sample(range(10), 5)\n",
    "rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [ground_truth[i] for i in range(len(ground_truth)) if i not in rm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ref_graph.nodes[i]['coord'] for i in range(len(ref_graph.nodes)) if i in rm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_coords = list(nx.get_node_attributes(ref_graph,'coord').values())\n",
    "\n",
    "dummy_coords = np.array([0.0,0.0,0.0])\n",
    "\n",
    "noisy_coords_2 = [dummy_coords if i in rm else noisy_coords[i] for i in range(len(noisy_coords))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.get_node_attributes(noisy_graph,'coord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy_graph.add_nodes_from([(13, {'is_dummy': True}),(14, {'is_dummy': True}),(15, {'is_dummy': True})])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisy_graph.add_node(16,is_dummy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in ground_truth:\n",
    "#     print(noisy_graph.nodes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt = np.load('ground_truth.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_graph = \"./noise_1000,outliers_20/graphs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g0 = nx.read_gpickle(path_graph+'graph_12.gpickle')\n",
    "# g1 = nx.read_gpickle(path_graph+'graph_1.gpickle')\n",
    "# g8 = nx.read_gpickle(path_graph+'graph_8.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gt_12 = {}\n",
    "\n",
    "# for i in range(len(gt_1)):\n",
    "#     for j in range(len(gt_2)):\n",
    "#         if gt_1[i] == gt_2[j]:\n",
    "#             gt_12[i] = j\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,j in gt_12.items():\n",
    "#     print(noisy_graph.nodes[i]['coord'], noisy_graph_1.nodes[j]['coord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for i,j in enumerate(ground_truth_1):\n",
    "#     print(noisy_graph_1.nodes[i]['coord'],noisy_coord_all[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_updated = nx.read_gpickle('g_updated.gpickle')\n",
    "g1_updated = nx.read_gpickle('g1_updated.gpickle')\n",
    "g2_updated = nx.read_gpickle('g2_updated.gpickle')\n",
    "g3_updated = nx.read_gpickle('g3_updated.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[87.31553887 46.75129557 13.795399  ] [88.69870357 44.10880515 13.67308647]\n",
      "7 .... 7\n",
      "[-54.6560096   83.71995267  -1.92097352] [-48.66965218  87.15523136  -5.93553732]\n",
      "3 .... 3\n",
      "[ 66.19227483 -67.74745147 -32.07593448] [ 64.90589069 -67.09576339 -35.85225082]\n",
      "6 .... 6\n",
      "[ 2.43701612 84.64590599 53.18958123] [ 3.92226246 82.34876436 56.59767545]\n",
      "9 .... 9\n",
      "[-40.18879963 -42.20336325 -81.26337745] [-40.03678482 -43.75260176 -80.51562395]\n",
      "1 .... 1\n"
     ]
    }
   ],
   "source": [
    "g_values = nx.get_node_attributes(g1_updated,'label_gt').values()\n",
    "\n",
    "for i,j in enumerate(g_values):\n",
    "    \n",
    "    if(j == -1):\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        print(g_updated.nodes[j]['coord'],g1_updated.nodes[i]['coord'])\n",
    "        print(g_updated.nodes[j]['label_gt'],'....',g1_updated.nodes[i]['label_gt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_graphs = [len(g_updated.nodes),len(g1_updated.nodes),len(g2_updated.nodes),len(g3_updated.nodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_graphs.index(min(list_graphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 6, 10, 4]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1_updated.nodes[4]['label_gt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeDataView({0: {'coord': array([88.69870357, 44.10880515, 13.67308647]), 'is_dummy': False, 'label_gt': 7}, 1: {'coord': array([-48.66965218,  87.15523136,  -5.93553732]), 'is_dummy': False, 'label_gt': 3}, 2: {'coord': array([ 64.90589069, -67.09576339, -35.85225082]), 'is_dummy': False, 'label_gt': 6}, 3: {'coord': array([ 3.92226246, 82.34876436, 56.59767545]), 'is_dummy': False, 'label_gt': 9}, 4: {'coord': array([-59.24975139,  73.60699077,  32.73343658]), 'is_dummy': False, 'label_gt': -1}, 5: {'coord': array([-40.03678482, -43.75260176, -80.51562395]), 'is_dummy': False, 'label_gt': 1}})"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g1_updated.nodes.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if there is -1 in attribute\n",
    "\n",
    "#number of nodes to add in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_label = list(nx.get_node_attributes(g1_updated,'label_gt').values())\n",
    "count_val = gt_label.count(-1)\n",
    "num_nodes_to_add = len(g_updated.nodes())-(len(g1_updated.nodes)-count_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  1,  3,  6,  7,  9])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(gt_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(g_updated.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_g1 = len(g1_updated.nodes)\n",
    "\n",
    "labels = list(set(list(g_updated.nodes)).symmetric_difference(set(gt_label)))\n",
    "\n",
    "if -1 in labels:\n",
    "    labels.remove(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 5, 8, 10, 11, 12, 13, 14, 15, 16, 17]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in enumerate(labels):\n",
    "    \n",
    "    g1_updated.add_node(len_g1+i,label_gt=j,coord=[-0.0,-0.0,-0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15,\n",
       "       16, 17])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(list(nx.get_node_attributes(g1_updated,'label_gt').values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_for_visu(g,g1):\n",
    "    \n",
    "    gt_label = list(nx.get_node_attributes(g1,'label_gt').values())\n",
    "    \n",
    "    len_g1 = len(g1.nodes)\n",
    "\n",
    "    \n",
    "    labels = list(set(list(g.nodes)).symmetric_difference(set(gt_label)))\n",
    "\n",
    "    if -1 in labels:\n",
    "        labels.remove(-1)\n",
    "        \n",
    "    for i,j in enumerate(labels):\n",
    "        \n",
    "        g1.add_node(len_g1+i,label_gt=j,coord=[-0.0,-0.0,-0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_for_visu(g_updated,g1_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 3, 6, 9, -1, 1, 0, 2, 4, 5, 8, 10, 11, 12, 13, 14, 15, 16, 17]"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nx.get_node_attributes(g1_updated,'label_gt').values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_0 = nx.read_gpickle('./noise_1400,outliers_20/graphs/graph_0.gpickle')\n",
    "graph_1 = nx.read_gpickle('./noise_1400,outliers_20/graphs/graph_1.gpickle')\n",
    "graph_2 = nx.read_gpickle('./noise_1400,outliers_20/graphs/graph_2.gpickle')\n",
    "graph_3 = nx.read_gpickle('./noise_1400,outliers_20/graphs/graph_3.gpickle')\n",
    "\n",
    "grt_01 = np.load('./noise_1400,outliers_20/ground_truth.gpickle',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_groundtruth = [grt_01['00'], grt_01['01'], grt_01['02'], grt_01['03']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_graphs = [graph_0,graph_1,graph_2,graph_3]\n",
    "np.save('new_groundtruth.npy',new_groundtruth)\n",
    "nx.write_gpickle(test_graphs,'test_graphs.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeView((0, 1, 2, 3, 4, 5, 6, 7))"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_3.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
